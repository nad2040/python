{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf27bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCE pytorch implementation\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "args = {\"gamma\": 0.99, \"seed\": 543, \"render\": True, \"log_interval\": 10}\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = deque()\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + args['gamma'] * R\n",
    "        returns.appendleft(R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    step_counts = []\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            if args['render']:\n",
    "                env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                step_counts.append(t)\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % args['log_interval'] == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "    plt.plot(step_counts)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7360f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2784aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84262f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2d845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0853a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5f7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pytorch Actor Critic with episodic updates and shared first linear layer\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Cart Pole\n",
    "\n",
    "args = {\"gamma\": 0.99, \"seed\": 543, \"render\": True, \"log_interval\": 10}\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "\n",
    "        # actor's layer\n",
    "        self.action_head = nn.Linear(128, 2)\n",
    "\n",
    "        # critic's layer\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "        # action & reward buffer\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward of both actor and critic\n",
    "        \"\"\"\n",
    "        x = F.relu(self.affine1(x))\n",
    "\n",
    "        # actor: choses action to take from state s_t\n",
    "        # by returning probability of each action\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "\n",
    "        # critic: evaluates being in the state s_t\n",
    "        state_values = self.value_head(x)\n",
    "\n",
    "        # return values for both actor and critic as a tuple of 2 values:\n",
    "        # 1. a list with the probability of each action over the action space\n",
    "        # 2. the value from state s_t\n",
    "        return action_prob, state_values\n",
    "\n",
    "\n",
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(state)\n",
    "\n",
    "    # create a categorical distribution over the list of probabilities of actions\n",
    "    m = Categorical(probs)\n",
    "\n",
    "    # and sample an action using the distribution\n",
    "    action = m.sample()\n",
    "\n",
    "    # save to action buffer\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "\n",
    "    # the action to take (left or right)\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    \"\"\"\n",
    "    Training code. Calculates actor and critic loss and performs backprop.\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = [] # list to save actor (policy) loss\n",
    "    value_losses = [] # list to save critic (value) loss\n",
    "    returns = [] # list to save the true values\n",
    "\n",
    "    # calculate the true value using rewards returned from the environment\n",
    "    for r in model.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + args['gamma'] * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "\n",
    "        # calculate actor (policy) loss\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "\n",
    "        # calculate critic (value) loss using L1 smooth loss\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "\n",
    "    # perform backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset rewards and action buffer\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "\n",
    "    # run infinitely many episodes\n",
    "    for i_episode in count(1):\n",
    "\n",
    "        # reset environment and episode reward\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        # for each episode, only run 9999 steps so that we don't\n",
    "        # infinite loop while learning\n",
    "        for t in range(1, 10000):\n",
    "\n",
    "            # select action from policy\n",
    "            action = select_action(state)\n",
    "\n",
    "            # take the action\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if args['render']:\n",
    "                env.render()\n",
    "\n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # update cumulative reward\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # perform backprop\n",
    "        finish_episode()\n",
    "\n",
    "        # log results\n",
    "        if i_episode % args['log_interval'] == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "\n",
    "        # check if we have \"solved\" the cart pole problem\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80388bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd69d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4df6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c02ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37575c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad317673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f4c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67567967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c328ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aaa10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8f49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea9f166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MY edit to pytorch actor critic implementation. split the model to two distinct networks.\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "args = {\"gamma\": 0.99, \"seed\": 543, \"render\": True, \"log_interval\": 10}\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "class A2C(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(A2C, self).__init__()\n",
    "        \n",
    "        ## pytorch example above has same first linear layer but two different linear outputs.\n",
    "        ## mine just separates it and performs better?\n",
    "        self.actor1 = nn.Linear(4, 128)\n",
    "        self.actor2 = nn.Linear(128, 2)\n",
    "        \n",
    "        self.critic1 = nn.Linear(4, 128)\n",
    "        self.critic2 = nn.Linear(128, 1)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        policy_prob_dist = F.softmax(self.actor2(F.relu(self.actor1(x))), dim=1)\n",
    "        value = self.critic2(F.relu(self.critic1(x)))\n",
    "        \n",
    "        return policy_prob_dist, value\n",
    "\n",
    "\n",
    "model = A2C()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    action_probs, state_value = model(state)\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample()\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = [] # list to save actor (policy) loss\n",
    "    value_losses = [] # list to save critic (value) loss\n",
    "    returns = deque()\n",
    "    \n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + args['gamma'] * R\n",
    "        returns.appendleft(R)\n",
    "    \n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    \n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "    loss.backward()\n",
    "#     retain_graph=True\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    step_counts = []\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            if args['render']:\n",
    "                env.render()\n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                step_counts.append(t)\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % args['log_interval'] == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "    plt.plot(step_counts)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e09440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22692afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08accc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911317a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d15db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c96830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6407da83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26881c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76073d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4685868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25cfd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MY implementation of actor critic that follows the algorithm under\n",
    "### One-step Actor–Critic (episodic)\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(Actor, self).__init__()\n",
    "        self.actor1 = nn.Linear(env.observation_space.shape[0] , 128)\n",
    "        self.actor2 = nn.Linear(128, env.action_space.n)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.actor2(torch.relu(self.actor1(x))), dim=-1)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(Critic, self).__init__()\n",
    "        self.critic1 = nn.Linear(env.observation_space.shape[0] , 128)\n",
    "        self.critic2 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.critic2(torch.relu(self.critic1(x)))\n",
    "\n",
    "args = {\"gamma\": 0.99, \"seed\": 543, \"render\": True, \"log_interval\": 10, \"actor_lr\": 1e-3, \"critic_lr\": 1e-3}\n",
    "gamma = args['gamma']\n",
    "seed = args['seed']\n",
    "render = args['render']\n",
    "log_interval = args['log_interval']\n",
    "actor_lr = args['actor_lr']\n",
    "critic_lr = args['critic_lr']\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "actor = Actor(env)\n",
    "critic = Critic(env)\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "\n",
    "def policy(state):\n",
    "    action_probs = actor(state)\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample()\n",
    "    return action, m.log_prob(action)\n",
    "\n",
    "def main():\n",
    "    step_counts = []\n",
    "    last_actor_losses = []\n",
    "    last_critic_losses = []\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        S, _ = env.reset()\n",
    "        S = torch.tensor(S, requires_grad=True).float().unsqueeze(0)\n",
    "        \n",
    "        ep_reward = 0\n",
    "        I = 1\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            A, log_prob = policy(S)\n",
    "            S2, R, done, _, _ = env.step(A.item())\n",
    "            S2 = torch.tensor(S2, requires_grad=True).float().unsqueeze(0)\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "            ep_reward += R\n",
    "            \n",
    "            error = R + (1.0 - done) * gamma * critic(S2) - critic(S)\n",
    "            error.requires_grad_()\n",
    "\n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss = - log_prob * error #* I\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            actor_optim.step()\n",
    "            \n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss = torch.square(error)\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            critic_optim.step()\n",
    "            \n",
    "            I = gamma * I\n",
    "            S = S2\n",
    "            \n",
    "            if done:\n",
    "                step_counts.append(t)\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "    plt.plot(step_counts)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa00384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c06cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26cb4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f785dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed52ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c5ff50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c84e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MY implementation of actor critic that follows the algorithm under\n",
    "### One-step Actor–Critic (episodic)\n",
    "\n",
    "import argparse\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2318ff-f4de-4984-95b3-e838f40eeb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"gamma\": 0.995, \"seed\": 1, \"render\": True, \"log_interval\": 50, \"actor_lr\": 1e-2, \"critic_lr\": 1e-2}\n",
    "gamma = args['gamma']\n",
    "seed = args['seed']\n",
    "render = args['render']\n",
    "log_interval = args['log_interval']\n",
    "actor_lr = args['actor_lr']\n",
    "critic_lr = args['critic_lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f8ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "num_state = env.observation_space.shape[0]\n",
    "num_action = env.action_space.n\n",
    "float_epsilon = np.finfo(np.float32).eps.item()\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(Actor, self).__init__()\n",
    "        self.actor1 = nn.Linear(num_state, 128)\n",
    "        self.actor2 = nn.Linear(128, num_action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.actor1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.actor2(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(Critic, self).__init__()\n",
    "        self.critic1 = nn.Linear(num_state, 128)\n",
    "        self.critic2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.critic1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.critic2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e9176-7ef3-4553-a213-d9acda1cb3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(actor, state):\n",
    "    action_probs = actor(state)\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample()\n",
    "    return action, m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed37d958-2d6e-424c-90e2-890f7ec39a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(steps, avg_steps):\n",
    "    ax = plt.subplot(111)\n",
    "    ax.cla()\n",
    "    ax.grid()\n",
    "    ax.set_title('Training')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Run Time')\n",
    "    ax.plot(steps)\n",
    "    ax.plot(avg_steps)\n",
    "    RunTime = len(steps)\n",
    "    # if len(steps) % 200 == 0:\n",
    "    #     path = './AC_MountainCar-v0/' + 'RunTime' + str(RunTime) + '.jpg'\n",
    "    #     plt.savefig(path)\n",
    "    plt.pause(0.0000001)\n",
    "\n",
    "def finish_episode(values, log_probs, rewards, actor, actor_optim, critic, critic_optim):\n",
    "    # Figure out returns\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in rewards[::-1]:\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "\n",
    "    # Normalize the returns\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + float_epsilon)\n",
    "    \n",
    "    # Figure out loss\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    for value, log_prob, g in zip(values, log_probs, returns):\n",
    "        advantage = g - value.item() # g = r + gamma * g' | g' is not bootstrapping V(S2) but it works\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([[g]])))\n",
    "\n",
    "    # Backpropagate\n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss = torch.stack(policy_losses).sum()\n",
    "    actor_loss.backward(retain_graph=True)\n",
    "    actor_optim.step()\n",
    "\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss = torch.stack(value_losses).sum()\n",
    "    critic_loss.backward(retain_graph=True)\n",
    "    critic_optim.step()\n",
    "\n",
    "    del values[:]\n",
    "    del log_probs[:]\n",
    "    del rewards[:]\n",
    "\n",
    "def train():\n",
    "    actor = Actor(env)\n",
    "    critic = Critic(env)\n",
    "    actor_optim = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    steps = []\n",
    "    avg_steps = []\n",
    "\n",
    "    # IMPORTANT: Using a replay buffer finally made Actor-Critic work, and it is much faster :)\n",
    "    values = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in count(1):\n",
    "        S, _ = env.reset()\n",
    "        S = torch.tensor(S, requires_grad=True).float().unsqueeze(0)\n",
    "        \n",
    "        for t in count(1):\n",
    "            value = critic(S)\n",
    "            values.append(value)\n",
    "            \n",
    "            A, log_prob = policy(actor, S)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            S2, R, done, _, _ = env.step(A.item())\n",
    "            S2 = torch.tensor(S2, requires_grad=True).float().unsqueeze(0)\n",
    "            rewards.append(R)\n",
    "            \n",
    "            S = S2\n",
    "\n",
    "            if done or t > 50000:\n",
    "                steps.append(t)\n",
    "                avg_steps.append(np.mean(steps[-min(100,len(steps)):]))\n",
    "\n",
    "                print(f\"Episode: {episode}\\tSteps {t}\")\n",
    "                break\n",
    "\n",
    "        finish_episode(values, log_probs, rewards, actor, actor_optim, critic, critic_optim)\n",
    "        plot(steps, avg_steps)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            path = f'./AC_MountainCar-v0_Model/actor{episode}.pth'\n",
    "            torch.save(actor, path)\n",
    "            path = f'./AC_MountainCar-v0_Model/critic{episode}.pth'\n",
    "            torch.save(critic, path)\n",
    "\n",
    "        if avg_steps[-1] < 140:\n",
    "            print(f\"Took {episode} episodes to reach 100 episode moving avg of {avg_steps[-1]} time steps.\")\n",
    "            path = f'./AC_MountainCar-v0_Model/actor{episode}.pth'\n",
    "            torch.save(actor, path)\n",
    "            path = f'./AC_MountainCar-v0_Model/critic{episode}.pth'\n",
    "            torch.save(critic, path)\n",
    "            break\n",
    "    \n",
    "    del steps\n",
    "    del avg_steps\n",
    "    del values\n",
    "    del log_probs\n",
    "    del rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c507a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b348aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count = 3065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba9aca-bb2f-4233-875d-91e0ce55ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(avg_episode_len_target=200, check_episode=200, new_lr=1e-4, quit=float(\"inf\")):\n",
    "    global episode_count\n",
    "    global log_interval\n",
    "    \n",
    "    env = gym.make('MountainCar-v0')\n",
    "    \n",
    "    print(f\"loading {episode_count} episode model\")\n",
    "    actor = torch.load(f'./AC_MountainCar-v0_Model/actor{episode_count}.pth')\n",
    "    actor.train()\n",
    "    critic = torch.load(f'./AC_MountainCar-v0_Model/critic{episode_count}.pth')\n",
    "    critic.train()\n",
    "\n",
    "    actor_optim = optim.Adam(actor.parameters(), lr=new_lr)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=new_lr)\n",
    "    \n",
    "    steps = []\n",
    "    avg_steps = []\n",
    "\n",
    "    values = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in count(episode_count+1):\n",
    "        S, _ = env.reset()\n",
    "        S = torch.tensor(S, requires_grad=True).float().unsqueeze(0)\n",
    "        \n",
    "        for t in count(1):\n",
    "            value = critic(S)\n",
    "            values.append(value)\n",
    "            \n",
    "            A, log_prob = policy(actor, S)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            S2, R, done, _, _ = env.step(A.item())\n",
    "            S2 = torch.tensor(S2, requires_grad=True).float().unsqueeze(0)\n",
    "            rewards.append(R)\n",
    "            \n",
    "            S = S2\n",
    "\n",
    "            if done or t > 50000:\n",
    "                steps.append(t)\n",
    "                avg_steps.append(np.mean(steps[-min(check_episode,len(steps)):]))\n",
    "\n",
    "                if episode % log_interval == 0:\n",
    "                    print(f\"Episode: {episode}\\tSteps {t}\")\n",
    "                    plot(steps, avg_steps)\n",
    "                break\n",
    "        \n",
    "        if t > 50000:\n",
    "            del values[:]\n",
    "            del log_probs[:]\n",
    "            del rewards[:]\n",
    "        else:\n",
    "            finish_episode(values, log_probs, rewards, actor, actor_optim, critic, critic_optim)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            path = f'./AC_MountainCar-v0_Model/actor{episode}.pth'\n",
    "            torch.save(actor, path)\n",
    "            path = f'./AC_MountainCar-v0_Model/critic{episode}.pth'\n",
    "            torch.save(critic, path)\n",
    "            \n",
    "        if episode - episode_count > quit:\n",
    "            plot(steps, avg_steps)\n",
    "            print(f\"stopping after {quit} episodes\")\n",
    "            path = f'./AC_MountainCar-v0_Model/actor{episode}.pth'\n",
    "            torch.save(actor, path)\n",
    "            path = f'./AC_MountainCar-v0_Model/critic{episode}.pth'\n",
    "            torch.save(critic, path)\n",
    "            episode_count = episode\n",
    "            print(f\"Current episode count is {episode_count} and the lr was {new_lr}\")\n",
    "            break\n",
    "        \n",
    "        if len(steps) > check_episode and avg_steps[-1] < avg_episode_len_target:\n",
    "            plot(steps, avg_steps)\n",
    "            print(f\"Took another {len(steps)} episodes to have a avg episode length of {avg_steps[-1]} in the past {check_episode} episodes\")\n",
    "            path = f'./AC_MountainCar-v0_Model/actor{episode}.pth'\n",
    "            torch.save(actor, path)\n",
    "            path = f'./AC_MountainCar-v0_Model/critic{episode}.pth'\n",
    "            torch.save(critic, path)\n",
    "            episode_count = episode\n",
    "            print(f\"Current episode count is {episode_count} and the lr was {new_lr}\")\n",
    "            break\n",
    "    \n",
    "    del steps\n",
    "    del avg_steps\n",
    "    del values\n",
    "    del log_probs\n",
    "    del rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a703fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(n_episode, num_eps=1000):\n",
    "    print(f\"loading {n_episode} episode model\")\n",
    "    actor = torch.load(f'./AC_MountainCar-v0_Model/actor{n_episode}.pth')\n",
    "    actor.eval()\n",
    "\n",
    "    steps = []\n",
    "    avg_steps = []\n",
    "\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    for episode in range(num_eps):\n",
    "        S, _ = env.reset()\n",
    "        S = torch.tensor(S, requires_grad=True).float().unsqueeze(0)\n",
    "\n",
    "        for t in count(1):\n",
    "            A, _ = policy(actor, S)\n",
    "            S2, _, done, _, _ = env.step(A.item())\n",
    "            S2 = torch.tensor(S2, requires_grad=True).float().unsqueeze(0)\n",
    "\n",
    "            S = S2\n",
    "\n",
    "            if done:\n",
    "                steps.append(t)\n",
    "                avg_steps.append(np.mean(steps))\n",
    "\n",
    "                print(f\"Episode: {episode}\\tSteps {t}\")\n",
    "                break\n",
    "    \n",
    "    print('steps',steps)\n",
    "    print('mean',avg_steps[-1])\n",
    "    print('fails',sum(1 for x in steps if x >= 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7bbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(3065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace6412-aec0-43b5-aa27-c63f51b942d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2(avg_episode_len_target=130,check_episode=200,new_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9120b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(6452)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e42f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2(avg_episode_len_target=130,check_episode=200,new_lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(6653)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7908f0b-19d9-476e-9d96-00ac44411512",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2(avg_episode_len_target=130,check_episode=200,new_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(6955)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d335fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count = 6955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d613112",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2(avg_episode_len_target=130,check_episode=200,new_lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b197e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(7883)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2(avg_episode_len_target=130,check_episode=200,new_lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(8241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b04a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2(avg_episode_len_target=130,check_episode=200,new_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495725cf-708f-49aa-8c55-ab79380a2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(9994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2(avg_episode_len_target=130,check_episode=200,new_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(10619)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0dbbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2(avg_episode_len_target=130,check_episode=200,new_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d5046",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(12463)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c56f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2(avg_episode_len_target=130,check_episode=200,new_lr=7e-4,quit=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(episode_count,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf49686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 41518: 10 fails\n",
    "# 47749: 9 fails\n",
    "# 54181: 9 fails\n",
    "# 55990: 10 fails\n",
    "# 58603: 7 fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db770c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddf835a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9a4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e5380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718f527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human intuition solution: use velocity direction to determine policy\n",
    "import gymnasium as gym\n",
    "from itertools import count\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "lengths = []\n",
    "\n",
    "for eps in range(1000):\n",
    "    S, _ = env.reset()\n",
    "\n",
    "    for t in count(1):\n",
    "        A = 0 if S2[1] < 0 else 2\n",
    "        S2, R, terminated, _, _ = env.step(A)\n",
    "\n",
    "        if terminated:\n",
    "            lengths.append(t)\n",
    "            break\n",
    "\n",
    "        S = S2\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(lengths)\n",
    "print(sum(lengths)/len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a445045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
