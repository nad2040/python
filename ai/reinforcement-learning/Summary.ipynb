{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf09196",
   "metadata": {},
   "source": [
    "# Summer of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1fb94",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## What is Reinforcement?\n",
    "In Reinforcement Learning (RL), the concept of \"reinforcement\" refers to the process of an agent learning how to make decisions in an environment to maximize a cumulative reward over time. The agent learns through trial and error by interacting with the environment and observing the consequences of its actions. The goal of RL is to enable the agent to take actions that lead to favorable outcomes and avoid actions that result in negative outcomes.\n",
    "\n",
    "In the context of RL, there are a few key components involved:\n",
    "\n",
    "- Agent: The agent is the learner that interacts with the environment and makes decisions. The agent's goal is to learn a policy, a strategy that maps states to actions, in order to maximize its cumulative reward.\n",
    "- Environment: The environment is the external system with which the agent interacts. It provides feedback to the agent based on the agent's actions, and it also transitions between different states based on the agent's actions.\n",
    "- State: A state represents a particular configuration or situation of the environment. The agent's actions and the environment's responses depend on the current state. The agent's goal is to learn how to select actions that lead to desirable states and outcomes.\n",
    "- Action: An action is a decision made by the agent that affects the environment. The agent's task is to learn a policy that determines which actions to take in different states to maximize its expected cumulative reward.\n",
    "- Reward: A reward is a numerical value that provides feedback to the agent about the goodness or desirability of an action taken in a particular state. The agent's objective is to learn a policy that selects actions to maximize the cumulative reward it receives over time.\n",
    "\n",
    "The term \"reinforcement\" in RL refers to the process of encouraging or discouraging certain actions based on the rewards received from the environment. When an agent takes an action that leads to a positive outcome, it receives a higher reward, reinforcing the behavior. Conversely, if an action leads to a negative outcome, the agent receives a lower reward, discouraging that behavior. The agent's learning process involves updating its policy based on the observed rewards and experiences to make better decisions in the future.\n",
    "\n",
    "In summary, reinforcement in reinforcement learning refers to the process of learning and adapting an agent's behavior based on the rewards and punishments received from the environment. The agent's objective is to learn a policy that maximizes the cumulative reward it obtains over time.\n",
    "\n",
    "## What is Learning?\n",
    "In Reinforcement Learning (RL), \"learning\" refers to the process through which an agent improves its decision-making abilities by interacting with an environment. The goal of learning in RL is for the agent to discover a strategy, called a policy, that enables it to make informed decisions to maximize its cumulative reward over time. Learning in RL involves the agent gradually adapting its behavior based on the feedback it receives from the environment.\n",
    "\n",
    "**There are several key components:**\n",
    "- Exploration and Exploitation: At the heart of RL learning is the trade-off between exploration and exploitation. Exploration involves trying out different actions in order to discover the effects they have on the environment. Exploitation involves choosing actions that the agent believes will yield the highest reward based on its current knowledge. Balancing these two aspects is crucial for effective learning, as the agent needs to explore to discover better actions while also exploiting its current knowledge to make the best decisions.\n",
    "- Policy Learning: The central goal of RL is to learn a policy, which is a strategy that maps states to actions. The agent's learning process involves finding the policy that maximizes its expected cumulative reward over time. This can be achieved through various algorithms and techniques, such as dynamic programming, Monte Carlo methods, and temporal difference learning.\n",
    "- Value Function Learning: In addition to learning a policy, RL also often involves learning a value function. A value function estimates the expected cumulative reward starting from a particular state and following a given policy. Value functions help the agent assess the desirability of states and guide its decision-making.\n",
    "- Temporal Credit Assignment: Learning in RL involves assigning credit to actions that led to favorable outcomes and withholding credit from actions that resulted in negative outcomes. Temporal credit assignment is the process of attributing rewards and penalties to the correct actions taken earlier in a sequence of actions that led to the observed outcome.\n",
    "- Trial and Error: RL agents learn through trial and error. They interact with the environment, take actions, observe outcomes, and receive rewards or penalties based on their actions. By experiencing different states and their associated outcomes, agents learn which actions lead to better outcomes over time.\n",
    "- Updating the Policy: As the agent interacts with the environment, it accumulates experiences and uses them to update its policy and value function. Different learning algorithms employ various update rules to iteratively refine the agent's decision-making strategy.\n",
    "- Convergence and Optimization: The ultimate goal of RL learning is to converge to an optimal policy that maximizes the cumulative reward. Convergence implies that the agent's policy and value function stop changing significantly, indicating that the agent has learned the best strategy within its current knowledge.\n",
    "\n",
    "In summary, learning in reinforcement learning involves an agent adapting its behavior over time through exploration, exploitation, and interactions with the environment. The agent aims to learn a policy and possibly a value function that enable it to make decisions that lead to higher cumulative rewards. The learning process is iterative and involves refining the agent's strategy based on feedback and experiences from the environment.\n",
    "\n",
    "\n",
    "## Agent and Environment interaction\n",
    "- Reinforcement learning follows a Markov Decision Process (MDP). The agent makes an action at a state and the environment responds with the next state and reward.\n",
    "\n",
    "## What is a policy?\n",
    "- A policy determines how an agent chooses its actions. It is a mapping from states to actions. The policy could return a probability distribution for the actions that could be taken in a state.\n",
    "\n",
    "## What is value?\n",
    "- In Reinforcement Learning (RL), \"value\" refers to the measure of the expected cumulative reward that an agent can achieve when starting from a particular state and following a specific policy. The concept of value is central to RL because it provides a way for the agent to assess the desirability or goodness of different states and guides its decision-making process.\n",
    "- There are two measures of value, state values (V) and action values (Q). The measure the expected value of the return G given an agent follows policy $\\pi$\n",
    "\n",
    "## What is return and discounting?\n",
    "In Reinforcement Learning, \"return\" refers to the cumulative sum of rewards that an agent receives when it follows a particular sequence of actions in an environment. It is a measure of the total reward the agent can expect to obtain from a given starting state while following a certain policy. The concept of return is fundamental because it quantifies the agent's objective of maximizing its long-term cumulative reward.\n",
    "\n",
    "γ (gamma) is the discount factor, a value between 0 and 1. It determines the weight given to future rewards compared to immediate rewards. A smaller γ places more emphasis on immediate rewards, while a larger γ values long-term rewards more. It serves as a way to model the agent's preference for immediate vs. future rewards.\n",
    "- Modeling Uncertainty: In real-world scenarios, future rewards are uncertain. The discount factor models this uncertainty by making future rewards less valuable than immediate rewards. A smaller γ reflects a higher level of uncertainty about the future.\n",
    "- Encouraging Convergence: Discounting with γ<1 ensures that infinite reward sequences are properly bounded, making the learning process more stable and well-behaved. It encourages the return to converge and allows iterative algorithms to converge more efficiently.\n",
    "- Promoting Finite Horizon Learning: By discounting future rewards, RL naturally focuses on optimizing the agent's behavior over a finite horizon of time steps. This is particularly useful when dealing with scenarios where long-term predictions might be unreliable or irrelevant.\n",
    "- Temporal Difference Learning: In temporal difference (TD) learning methods, like Q-learning and SARSA, the discount factor is crucial for estimating the value or action-value functions. It helps the agent update its estimates based on the difference between predicted future rewards and observed immediate rewards.\n",
    "- Preference for Short-Term vs. Long-Term Rewards: The choice of γ allows the agent's behavior to be tuned to its preference for immediate rewards (small γ) or long-term cumulative rewards (large γ).\n",
    "\n",
    "## Goal\n",
    "Through reinforcement learning, we want to 'learn' the policy that maximizes expected return.\n",
    "\n",
    "Note: All methods below are for the episodic case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa63a7",
   "metadata": {},
   "source": [
    "# Dynamic Programming (DP)\n",
    "\n",
    "## Value and Bellman's Equations\n",
    "- The Bellman equations are a set of mathematical equations that describe the relationship between the value of a state or state-action pair and the values of its neighboring states or state-action pairs in a Markov Decision Process (MDP). These equations play a central role in reinforcement learning and dynamic programming algorithms for solving MDPs. There are two primary forms of the Bellman equations: the Bellman Expectation Equation and the Bellman Optimality Equation.\n",
    "\n",
    "1. **Bellman Expectation Equation (State-Value Function):**\n",
    "The state-value Bellman Expectation Equation describes the value of a state in terms of the expected cumulative reward obtained by following a certain policy after reaching that state:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r|s, a) \\left[ r + \\gamma V^\\pi(s') \\right]$$\n",
    "\n",
    "where:\n",
    "- $V^\\pi(s)$ is the value of state $s$ under policy $\\pi$, representing the expected cumulative reward starting from state $s$ and following policy $\\pi$.\n",
    "- $\\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\\pi$.\n",
    "- $p(s', r|s, a)$ is the probability of transitioning to state $s'$ and receiving reward $r$ when taking action $a$ in state $s$.\n",
    "- $\\gamma$ is the discount factor, balancing immediate and future rewards.\n",
    "\n",
    "2. **Bellman Expectation Equation (Action-Value Function):**\n",
    "The action-value Bellman Expectation Equation describes the value of a state-action pair in terms of the expected cumulative reward obtained by following a certain policy after taking the action at that state:\n",
    "\n",
    "$$Q^\\pi(s, a) = \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a' | s') Q^\\pi(s', a') \\right]$$\n",
    "\n",
    "where:\n",
    "- $Q^\\pi(s, a)$ represents the action-value function for state $s$ and action $a$ under policy $\\pi$.\n",
    "- $\\sum_{s', r}$ denotes the summation over all possible next states $s'$ and rewards $r$ that can be obtained when transitioning from state $s$ to state $s'$ after taking action $a$.\n",
    "- $p(s', r | s, a)$ is the probability of transitioning to state $s'$ and receiving reward $r$ when taking action $a$ in state $s$.\n",
    "- $r$ is the immediate reward obtained after taking action $a$ in state $s$ and transitioning to state $s'$.\n",
    "- $\\gamma$ is the discount factor, which balances immediate and future rewards.\n",
    "- $\\sum_{a'}$ represents the summation over all possible actions $a'$ that can be taken in the next state $s'$.\n",
    "- $\\pi(a' | s')$ is the probability of taking action $a'$ in state $s'$ under policy $\\pi$.\n",
    "- $Q^\\pi(s', a')$ is the action-value function for the next state $s'$ and action $a'$ under policy $\\pi$.\n",
    "\n",
    "3. **Bellman Optimality Equation (State-Value Function):**\n",
    "The Bellman Optimality Equation describes the optimal value of a state in terms of the maximum expected cumulative reward obtainable from that state:\n",
    "\n",
    "$$V^*(s) = \\max_a \\sum_{s', r} p(s', r|s, a) \\left[ r + \\gamma V^*(s') \\right]$$\n",
    "\n",
    "where:\n",
    "- $V^*(s)$ is the optimal value of state $s$, representing the maximum expected cumulative reward achievable starting from state $s$ under an optimal policy.\n",
    "- The other symbols have the same meanings as before.\n",
    "\n",
    "4. **Bellman Optimality Equation (Action-Value Function):**\n",
    "The Bellman Optimality Equation can also be expressed for the optimal action-value function $Q^*(s, a)$, which represents the maximum expected cumulative reward starting from state $s$, taking action $a$, and then following an optimal policy:\n",
    "\n",
    "$$Q^*(s, a) = \\sum_{s', r} p(s', r|s, a) \\left[ r + \\gamma \\max_{a'} Q^*(s', a') \\right]$$\n",
    "\n",
    "where:\n",
    "- $Q^*(s, a)$ is the optimal action-value function for state $s$ and action $a$.\n",
    "- The other symbols have the same meanings as before.\n",
    "\n",
    "The Bellman equations serve as a foundation for various reinforcement learning algorithms, such as Dynamic Programming, Value Iteration, and Q-learning. They provide a recursive way to express the value of states or state-action pairs in terms of the values of other states or state-action pairs, facilitating the computation of optimal policies and value functions in MDPs.\n",
    "\n",
    "\n",
    "### Bootstrapping\n",
    "Bootstrapping, in the context of Reinforcement Learning (RL) and many other fields, refers to a technique where an estimate or value is updated based on its own previous estimate. It involves using the current estimate as a starting point to refine or improve the estimate iteratively. Bootstrapping is widely used in RL algorithms, and it is an essential concept in temporal difference (TD) learning, such as in Q-learning, SARSA, and the Bellman equation.\n",
    "\n",
    "\n",
    "### Assumptions/Knowns\n",
    "- We have perfect knowledge of the environment. That means we have the state-transition probabilities.\n",
    "### Objective\n",
    "- A table of state values. The objective is to continually improve these value approximations until making moves greedily according to the values is the most optimal strategy.\n",
    "### Process\n",
    "- Bellman's Equations used as update rules. There are two similar methods below.\n",
    "\n",
    "## Policy Iteration\n",
    "### Policy Evaluation\n",
    "In the first step, Policy Iteration evaluates the value function for a given policy. The value function of a policy represents the expected cumulative reward starting from each state and following the policy.\n",
    "\n",
    "It does so by solving the Bellman equation for the value function. The Bellman equation expresses the relationship between the value of a state and the values of its neighboring states under the given policy.\n",
    "\n",
    "By iteratively applying the Bellman equation, the value function is updated until it converges to the true value function of the policy.\n",
    "\n",
    "One loop through updating every state is called a 'sweep'\n",
    "\n",
    "### Policy Improvement\n",
    "Once the value function has converged, Policy Iteration then improves the policy greedily based on the value function.\n",
    "\n",
    "For each state, the agent selects the action that leads to the state with the highest value according to the value function. This is done to ensure that the agent chooses the action that maximizes the expected cumulative reward from each state.\n",
    "\n",
    "The process of policy improvement guarantees that the new policy is at least as good as the old policy (it may be even better), as it selects actions based on the currently known value function.\n",
    "\n",
    "### Policy Improvement Theorem\n",
    "The Policy Improvement Theorem is a fundamental result in reinforcement learning that establishes the conditions under which a new policy can be guaranteed to be at least as good as the old policy, or strictly better.\n",
    "\n",
    "The Policy Improvement Theorem states:\n",
    "\n",
    "Let $\\pi$ and $\\pi'$ be any two deterministic policies for an MDP (Markov Decision Process) such that, for all states s in the state space:\n",
    "\n",
    "$Q_{\\pi}(s, \\pi'(s)) \\geq V_{\\pi}(s)$\n",
    "\n",
    "where:\n",
    "- $\\pi$ is the old policy\n",
    "- $\\pi'$ is a new policy.\n",
    "- $Q_{\\pi}(s, \\pi'(s))$ is the action-value function of state s under policy π', representing the expected cumulative reward starting from state s, taking action π'(s), and then following policy π for subsequent actions.\n",
    "- $V_{\\pi}(s)$ is the value function of state s under policy π, representing the expected cumulative reward starting from state s and following policy π.\n",
    "\n",
    "In simpler terms, the Policy Improvement Theorem asserts that if for every state, the action selected by the new policy $\\pi'$ leads to a higher or equal expected cumulative reward than the current policy $\\pi$ would yield, then the new policy $\\pi'$ is guaranteed to be at least as good as the old policy $\\pi$.\n",
    "\n",
    "Moreover, if for at least one state, the action selected by the new policy $\\pi'$ leads to a strictly higher expected cumulative reward than the current policy $\\pi$, then the new policy $\\pi'$ is strictly better than the old policy $\\pi$.\n",
    "\n",
    "This theorem is at the core of policy iteration algorithms, such as the Policy Iteration and Value Iteration methods. Policy Iteration uses the Policy Improvement Theorem to ensure that the new policy in each iteration is at least as good as the old policy, ultimately leading to the convergence of the algorithm to an optimal policy. Value Iteration, a more efficient approach, uses the Policy Improvement Theorem implicitly in its update rule for the value function, which ensures that the greedy policy improvement step will yield a better policy in each iteration.\n",
    "\n",
    "\n",
    "## Value Iteration\n",
    "- Value Iteration is a simpler and more efficient version of Policy Iteration, combining policy evaluation and policy improvement in a single step.\n",
    "- It updates the value function directly by iteratively computing the expected cumulative reward from each state for each possible action and choosing the action that maximizes the value function at each state. This process continues until the value function converges to the optimal value function.\n",
    "- The final optimal policy can be derived from the converged optimal value function by selecting the action that maximizes the value for each state.\n",
    "\n",
    "- All of these algorithms converge to an optimal policy for discounted finite MDPs. (GAMMA is important for convergence!!!)\n",
    "\n",
    "___\n",
    "\n",
    "Generalized Policy Iteration (GPI) is the idea that these two methods can generalize for other Reinforcement Learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dba979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ACTION_SPACE = ('U', 'D', 'L', 'R')\n",
    "REWARD_SPACE = (-1,)\n",
    "\n",
    "class Grid:  # Environment\n",
    "    def __init__(self, rows, cols, start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "    def set(self, terminal_rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.terminal_rewards = terminal_rewards\n",
    "        self.actions = actions\n",
    "\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    def reset(self):\n",
    "        # put agent back in start position\n",
    "        self.i = 2\n",
    "        self.j = 0\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def get_next_state(self, s, a):\n",
    "        # this answers: where would I end up if I perform action 'a' in state 's'?\n",
    "        i, j = s[0], s[1]\n",
    "\n",
    "        # if this action moves you somewhere else, then it will be in this dictionary\n",
    "        if a in self.actions[(i, j)]:\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            elif a == 'D':\n",
    "                i += 1\n",
    "            elif a == 'R':\n",
    "                j += 1\n",
    "            elif a == 'L':\n",
    "                j -= 1\n",
    "        return i, j\n",
    "\n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.terminal_rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    def undo_move(self, action):\n",
    "        # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert (self.current_state() in self.all_states())\n",
    "\n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(self.actions.keys()) | set(self.terminal_rewards.keys())\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(4, 4, (2, 1))\n",
    "    terminal_rewards = {(0, 0): -1, (3, 3): -1}\n",
    "    actions = {\n",
    "            (0, 1): ('L', 'D', 'R'),\n",
    "            (0, 2): ('L', 'D', 'R'),\n",
    "            (0, 3): ('L', 'D'),\n",
    "            (1, 0): ('U', 'D', 'R'),\n",
    "            (1, 1): ('U', 'L', 'D', 'R'),\n",
    "            (1, 2): ('U', 'L', 'D', 'R'),\n",
    "            (1, 3): ('U', 'L', 'D'),\n",
    "            (2, 0): ('U', 'D', 'R'),\n",
    "            (2, 1): ('U', 'L', 'D', 'R'),\n",
    "            (2, 2): ('U', 'L', 'D', 'R'),\n",
    "            (2, 3): ('U', 'L', 'D'),\n",
    "            (3, 0): ('U', 'R'),\n",
    "            (3, 1): ('U', 'L', 'R'),\n",
    "            (3, 2): ('U', 'L', 'R')\n",
    "            }\n",
    "    g.set(terminal_rewards, actions)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "grid = standard_grid()\n",
    "print(grid.all_states())\n",
    "print(grid.actions)\n",
    "print(grid.terminal_rewards)\n",
    "\n",
    "def transition_probability_and_reward(grid):\n",
    "    transition = {}\n",
    "    for s in grid.actions.keys():\n",
    "        for a in ACTION_SPACE:\n",
    "            distribution = {}\n",
    "            for s2 in grid.all_states():\n",
    "                for r in REWARD_SPACE:\n",
    "                    # this code is special for the gridworld case\n",
    "                    # only one possible next state and one possible reward per step\n",
    "                    if s2 == grid.get_next_state(s, a) and r == -1:\n",
    "                        distribution[(s2, grid.terminal_rewards.get(s2, r))] = 1\n",
    "            transition[(s,a)] = distribution\n",
    "    return transition\n",
    "\n",
    "\n",
    "def print_values(V, g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.cols):\n",
    "            v = V.get((i, j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")  # -ve sign takes up an extra space\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.cols):\n",
    "            a = P.get((i, j), ' ')\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "\n",
    "THRESHOLD = 1e-3\n",
    "GAMMA = 0.9\n",
    "\n",
    "P = transition_probability_and_reward(grid)\n",
    "\n",
    "\n",
    "def initial_policy(grid):\n",
    "    policy = {}\n",
    "    for s in grid.actions.keys():\n",
    "        policy[s] = np.random.choice(ACTION_SPACE)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def policy_evaluation(grid, policy, V=None):\n",
    "    if V is None:\n",
    "        V = {}\n",
    "        for s in grid.all_states():\n",
    "            V[s] = 0\n",
    "\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in grid.actions.keys():\n",
    "            v = V[s]\n",
    "\n",
    "            # deterministic policy, no outer sum\n",
    "            a = policy[s]\n",
    "            updated_value = 0\n",
    "            for s2 in grid.all_states():\n",
    "                for r in REWARD_SPACE:\n",
    "                    updated_value += P[(s,a)].get((s2, r), 0) * (r + GAMMA * V[s2])\n",
    "\n",
    "            V[s] = updated_value\n",
    "\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "\n",
    "        print_values(V, grid)\n",
    "        iterations += 1\n",
    "\n",
    "        if delta < THRESHOLD:\n",
    "            break\n",
    "\n",
    "    print(f\"evaluation took {iterations} iterations\")\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improvement(grid, policy, state_valuation):\n",
    "    policy_stable = True\n",
    "\n",
    "    for s in grid.actions.keys():\n",
    "        old_action = policy[s]\n",
    "\n",
    "        action_values = []\n",
    "        for a in ACTION_SPACE:\n",
    "            s2 = grid.get_next_state(s, a)\n",
    "            # in this deterministic policy, state value is action value?\n",
    "            action_values.append(state_valuation.get(s2))\n",
    "        policy[s] = ACTION_SPACE[np.argmax(action_values)]\n",
    "\n",
    "        if old_action != policy[s]:\n",
    "            policy_stable = False\n",
    "\n",
    "    return policy_stable\n",
    "\n",
    "\n",
    "def policy_iteration(grid):\n",
    "    policy = initial_policy(grid)\n",
    "    state_values = None\n",
    "\n",
    "    print_policy(policy, grid)\n",
    "\n",
    "    improvement_count = 0\n",
    "    while True:\n",
    "        print(\"policy evaluation\")\n",
    "        state_values = policy_evaluation(grid, policy, state_values)\n",
    "\n",
    "        print(\"policy improvement\")\n",
    "        policy_stable = policy_improvement(grid, policy, state_values)\n",
    "        print_policy(policy, grid)\n",
    "\n",
    "        improvement_count += 1\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    print(f\"policy iteration took {improvement_count} iterations\")\n",
    "    return state_values, policy\n",
    "\n",
    "policy_iteration(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ACTION_SPACE = ('U', 'D', 'L', 'R')\n",
    "REWARD_SPACE = (-1, 0, 1)\n",
    "\n",
    "class Grid:  # Environment\n",
    "    def __init__(self, rows, cols, start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "    def set(self, terminal_rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.terminal_rewards = terminal_rewards\n",
    "        self.actions = actions\n",
    "\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    def reset(self):\n",
    "        # put agent back in start position\n",
    "        self.i = 2\n",
    "        self.j = 0\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def get_next_state(self, s, a):\n",
    "        # this answers: where would I end up if I perform action 'a' in state 's'?\n",
    "        i, j = s[0], s[1]\n",
    "\n",
    "        # if this action moves you somewhere else, then it will be in this dictionary\n",
    "        if a in self.actions[(i, j)]:\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            elif a == 'D':\n",
    "                i += 1\n",
    "            elif a == 'R':\n",
    "                j += 1\n",
    "            elif a == 'L':\n",
    "                j -= 1\n",
    "        return i, j\n",
    "\n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.terminal_rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    def undo_move(self, action):\n",
    "        # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert (self.current_state() in self.all_states())\n",
    "\n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(self.actions.keys()) | set(self.terminal_rewards.keys())\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    terminal_rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "            (0, 0): ('D', 'R'),\n",
    "            (0, 1): ('L', 'R'),\n",
    "            (0, 2): ('L', 'D', 'R'),\n",
    "            (1, 0): ('U', 'D'),\n",
    "            (1, 2): ('U', 'D', 'R'),\n",
    "            (2, 0): ('U', 'R'),\n",
    "            (2, 1): ('L', 'R'),\n",
    "            (2, 2): ('L', 'R', 'U'),\n",
    "            (2, 3): ('L', 'U'),\n",
    "            }\n",
    "    g.set(terminal_rewards, actions)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "grid = standard_grid()\n",
    "print(grid.all_states())\n",
    "print(grid.actions)\n",
    "print(grid.terminal_rewards)\n",
    "\n",
    "def transition_probability_and_reward(grid):\n",
    "    transition = {}\n",
    "    for s in grid.actions.keys():\n",
    "        for a in ACTION_SPACE:\n",
    "            distribution = {}\n",
    "            for s2 in grid.all_states():\n",
    "                for r in REWARD_SPACE:\n",
    "                    # this code is special for the gridworld case\n",
    "                    # only one possible next state and one possible reward per step\n",
    "                    if s2 == grid.get_next_state(s, a) and r == 0:\n",
    "                        distribution[(s2,r + grid.terminal_rewards.get(s2,0))] = 1\n",
    "            transition[(s,a)] = distribution\n",
    "    return transition\n",
    "\n",
    "\n",
    "def print_values(V, g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.cols):\n",
    "            v = V.get((i, j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")  # -ve sign takes up an extra space\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.cols):\n",
    "            a = P.get((i, j), ' ')\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "\n",
    "THRESHOLD = 1e-6\n",
    "GAMMA = 0.9\n",
    "\n",
    "P = transition_probability_and_reward(grid)\n",
    "\n",
    "\n",
    "def initial_policy(grid):\n",
    "    policy = {}\n",
    "    for s in grid.actions.keys():\n",
    "        policy[s] = np.random.choice(ACTION_SPACE)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def policy_evaluation(grid, policy, value_iteration=False):\n",
    "    V = {}\n",
    "    for s in grid.all_states():\n",
    "        V[s] = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in grid.actions.keys():\n",
    "            v = V[s]\n",
    "\n",
    "            # deterministic policy, no outer sum\n",
    "            a = policy[s]\n",
    "            updated_value = 0\n",
    "            for s2 in grid.all_states():\n",
    "                for r in REWARD_SPACE:\n",
    "                    updated_value += P[(s,a)].get((s2, r), 0) * (r + GAMMA * V[s2])\n",
    "\n",
    "            V[s] = updated_value\n",
    "\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "\n",
    "        print_values(V, grid)\n",
    "\n",
    "        if delta < THRESHOLD or value_iteration:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improvement(grid, policy, state_valuation):\n",
    "    policy_stable = True\n",
    "\n",
    "    for s in grid.actions.keys():\n",
    "        old_action = policy[s]\n",
    "\n",
    "        action_values = []\n",
    "        for a in grid.actions[s]:\n",
    "            s2 = grid.get_next_state(s, a)\n",
    "            # in this deterministic policy, state value is action value?\n",
    "            action_values.append((state_valuation[s2] + grid.terminal_rewards.get(s2,0),a))\n",
    "        policy[s] = max(action_values)[1]\n",
    "\n",
    "        if old_action != policy[s]:\n",
    "            policy_stable = False\n",
    "\n",
    "    return policy, policy_stable\n",
    "\n",
    "\n",
    "def policy_iteration(grid):\n",
    "    policy = initial_policy(grid)\n",
    "    state_values = {}\n",
    "\n",
    "    print_policy(policy, grid)\n",
    "\n",
    "    while True:\n",
    "        print(\"policy evaluation\")\n",
    "        state_values = policy_evaluation(grid, policy)\n",
    "        print_values(state_values, grid)\n",
    "\n",
    "        print(\"policy improvement\")\n",
    "        policy, policy_stable = policy_improvement(grid, policy, state_values)\n",
    "        print_policy(policy, grid)\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return state_values, policy\n",
    "\n",
    "\n",
    "policy_iteration(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34fc813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ACTION_SPACE = ('U', 'D', 'L', 'R')\n",
    "REWARD_SPACE = (-1,0)\n",
    "\n",
    "class Grid:  # Environment\n",
    "    def __init__(self, rows, cols, start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "    def set(self, terminal_rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.terminal_rewards = terminal_rewards\n",
    "        self.actions = actions\n",
    "\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    def reset(self):\n",
    "        # put agent back in start position\n",
    "        self.i = 2\n",
    "        self.j = 0\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def get_next_state(self, s, a):\n",
    "        # this answers: where would I end up if I perform action 'a' in state 's'?\n",
    "        i, j = s[0], s[1]\n",
    "\n",
    "        # if this action moves you somewhere else, then it will be in this dictionary\n",
    "        if a in self.actions[(i, j)]:\n",
    "            if a == 'U':\n",
    "                i -= 1\n",
    "            elif a == 'D':\n",
    "                i += 1\n",
    "            elif a == 'R':\n",
    "                j += 1\n",
    "            elif a == 'L':\n",
    "                j -= 1\n",
    "        return i, j\n",
    "\n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.terminal_rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    def undo_move(self, action):\n",
    "        # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert (self.current_state() in self.all_states())\n",
    "\n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    def all_states(self):\n",
    "        # possibly buggy but simple way to get all states\n",
    "        # either a position that has possible next actions\n",
    "        # or a position that yields a reward\n",
    "        return set(self.actions.keys()) | set(self.terminal_rewards.keys())\n",
    "\n",
    "\n",
    "def standard_grid():\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "    g = Grid(4, 4, (2, 1))\n",
    "    terminal_rewards = {(0, 0): -1, (3, 3): -1}\n",
    "    actions = {\n",
    "            (0, 1): ('L', 'D', 'R'),\n",
    "            (0, 2): ('L', 'D', 'R'),\n",
    "            (0, 3): ('L', 'D'),\n",
    "            (1, 0): ('U', 'D', 'R'),\n",
    "            (1, 1): ('U', 'L', 'D', 'R'),\n",
    "            (1, 2): ('U', 'L', 'D', 'R'),\n",
    "            (1, 3): ('U', 'L', 'D'),\n",
    "            (2, 0): ('U', 'D', 'R'),\n",
    "            (2, 1): ('U', 'L', 'D', 'R'),\n",
    "            (2, 2): ('U', 'L', 'D', 'R'),\n",
    "            (2, 3): ('U', 'L', 'D'),\n",
    "            (3, 0): ('U', 'R'),\n",
    "            (3, 1): ('U', 'L', 'R'),\n",
    "            (3, 2): ('U', 'L', 'R')\n",
    "            }\n",
    "    g.set(terminal_rewards, actions)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "grid = standard_grid()\n",
    "print(grid.all_states())\n",
    "print(grid.actions)\n",
    "print(grid.terminal_rewards)\n",
    "\n",
    "def transition_probability_and_reward(grid):\n",
    "    transition = {}\n",
    "    for s in grid.actions.keys():\n",
    "        for a in ACTION_SPACE:\n",
    "            distribution = {}\n",
    "            for s2 in grid.all_states():\n",
    "                for r in REWARD_SPACE:\n",
    "                    # this code is special for the gridworld case\n",
    "                    # only one possible next state and one possible reward per step\n",
    "                    if s2 == grid.get_next_state(s, a) and r == -1:\n",
    "                        distribution[(s2, grid.terminal_rewards.get(s2, r))] = 1\n",
    "            transition[(s,a)] = distribution\n",
    "    return transition\n",
    "\n",
    "\n",
    "def print_values(V, g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.cols):\n",
    "            v = V.get((i, j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")  # -ve sign takes up an extra space\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.rows):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.cols):\n",
    "            a = P.get((i, j), ' ')\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "\n",
    "THRESHOLD = 1e-3\n",
    "GAMMA = 1\n",
    "\n",
    "P = transition_probability_and_reward(grid)\n",
    "\n",
    "\n",
    "def initial_policy(grid):\n",
    "    policy = {}\n",
    "    for s in grid.actions.keys():\n",
    "        policy[s] = { a:1/len(ACTION_SPACE) for a in ACTION_SPACE }\n",
    "    return policy\n",
    "\n",
    "\n",
    "def policy_evaluation(grid, policy, value_iteration=False):\n",
    "    V = {}\n",
    "    for s in grid.all_states():\n",
    "        V[s] = 0\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in grid.actions.keys():\n",
    "            v = V[s]\n",
    "\n",
    "            updated_value = 0\n",
    "            for a in ACTION_SPACE:\n",
    "                action_value = 0\n",
    "                for s2 in grid.all_states():\n",
    "                    for r in REWARD_SPACE:\n",
    "                        action_value += P[(s,a)].get((s2, r), 0) * (r + GAMMA * V[s2])\n",
    "                updated_value += policy[s][a] * action_value\n",
    "\n",
    "            V[s] = updated_value\n",
    "\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "\n",
    "        print_values(V, grid)\n",
    "        iterations+=1\n",
    "        if delta < THRESHOLD or value_iteration:\n",
    "            break\n",
    "    print(f\"iterations: {iterations}\")\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improvement(grid, policy, state_valuation):\n",
    "    policy_stable = True\n",
    "\n",
    "    for s in grid.actions.keys():\n",
    "        old_action = policy[s]\n",
    "\n",
    "        action_values = []\n",
    "        for a in ACTION_SPACE:\n",
    "            s2 = grid.get_next_state(s, a)\n",
    "            # in this deterministic policy, state value is action value?\n",
    "            action_values.append((grid.terminal_rewards.get(s2,state_valuation[s2]),a))\n",
    "        policy[s] = max(action_values)[1]\n",
    "\n",
    "        if old_action != policy[s]:\n",
    "            policy_stable = False\n",
    "\n",
    "    return policy, policy_stable\n",
    "\n",
    "\n",
    "def policy_iteration(grid):\n",
    "    policy = initial_policy(grid)\n",
    "    state_values = {}\n",
    "\n",
    "    print_policy(policy, grid)\n",
    "\n",
    "    while True:\n",
    "        print(\"policy evaluation\")\n",
    "        state_values = policy_evaluation(grid, policy)\n",
    "        print_values(state_values, grid)\n",
    "\n",
    "        print(\"policy improvement\")\n",
    "        policy, policy_stable = policy_improvement(grid, policy, state_values)\n",
    "        print_policy(policy, grid)\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return state_values, policy\n",
    "\n",
    "\n",
    "def value_iteration(grid):\n",
    "    state_values = {}\n",
    "    for s in grid.all_states():\n",
    "        state_values[s] = 0\n",
    "\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in grid.actions.keys():\n",
    "            v = state_values[s]\n",
    "\n",
    "            action_values = []\n",
    "            for a in ACTION_SPACE:\n",
    "                s2 = grid.get_next_state(s, a)\n",
    "                action_value = 0\n",
    "                for s2 in grid.all_states():\n",
    "                    for r in REWARD_SPACE:\n",
    "                        action_value += P[(s,a)].get((s2, r), 0) * (r + GAMMA * state_values[s2])\n",
    "                action_values.append(action_value)\n",
    "            state_values[s] = max(action_values)\n",
    "\n",
    "            delta = max(delta, abs(v - state_values[s]))\n",
    "\n",
    "        print_values(state_values, grid)\n",
    "        iterations+=1\n",
    "        if delta < THRESHOLD:\n",
    "            break\n",
    "    print(f\"iterations: {iterations}\")\n",
    "\n",
    "    policy = {}\n",
    "    for s in grid.actions.keys():\n",
    "        action_values = []\n",
    "        for a in ACTION_SPACE:\n",
    "            s2 = grid.get_next_state(s, a)\n",
    "            action_value = 0\n",
    "            for s2 in grid.all_states():\n",
    "                for r in REWARD_SPACE:\n",
    "                    action_value += P[(s,a)].get((s2, r), 0) * (r + GAMMA * state_values[s2])\n",
    "            action_values.append(action_value)\n",
    "\n",
    "        policy[s] = ACTION_SPACE[action_values.index(max(action_values))]\n",
    "\n",
    "    print_policy(policy, grid)\n",
    "\n",
    "\n",
    "# policy_iteration(grid)\n",
    "#\n",
    "# policy = initial_policy(grid)\n",
    "# print(policy)\n",
    "# state_values = policy_evaluation(grid, policy)\n",
    "#\n",
    "value_iteration(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae77022",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods (MC)\n",
    "\n",
    "### Assumptions/Knowns\n",
    "- Monte Carlo methods do not require complete knowledge of the MDP.\n",
    "### Objective\n",
    "- A table of action values. The objective is to continually improve these action value approximations until making moves greedily according to the values is the most optimal strategy.\n",
    "### Process\n",
    "- Repeatedly\n",
    "  - simulate an episode\n",
    "  - keep track of rewards at each step following (state,action) pairs\n",
    "  - average the rewards\n",
    "\n",
    "## Monte Carlo Exploring Starts (First-Visit)\n",
    "Monte Carlo ES (Exploring Starts) is a reinforcement learning algorithm that combines the Monte Carlo method with an exploration strategy called \"exploring starts.\" This algorithm is used to find the optimal policy for Markov Decision Processes (MDPs) in which the agent can explore different state-action pairs while still guaranteeing that each state-action pair is visited infinitely often.\n",
    "\n",
    "The key idea behind Monte Carlo ES is to ensure that the agent explores all possible state-action pairs by starting episodes from random states and taking random actions. By using exploring starts, the agent can learn about the value of state-action pairs even if the original policy would not have reached those pairs due to a lack of exploration.\n",
    "\n",
    "- Exploring Starts: The algorithm starts each episode from a randomly chosen state-action pair. This guarantees that all state-action pairs have a non-zero probability of being selected as a starting point for an episode.\n",
    "- Data Collection: The agent interacts with the environment, following the policy that was initialized with the exploring starts. It collects trajectories (sequences of states, actions, and rewards) during each episode.\n",
    "- Estimate Action Values: For each state-action pair encountered in the episodes, Monte Carlo ES estimates the action-value function (Q-function) by averaging the returns obtained when taking that action from that state.\n",
    "- Policy Improvement: The agent updates its policy based on the estimated action values. It chooses actions that maximize the estimated action values to improve its policy.\n",
    "- Iteration: The above steps are repeated for multiple episodes, with the policy being improved iteratively based on the collected experience and the estimated action values.\n",
    "\n",
    "Monte Carlo ES ensures that the agent explores a wide range of state-action pairs, making it a useful method for environments with a large or complex state-action space. By using the exploring starts strategy, the agent can learn about the value of state-action pairs that might be critical for finding an optimal policy but would be under-explored using a purely greedy or epsilon-greedy policy.\n",
    "\n",
    "### First-Visit\n",
    "What First Visit MC means is that in each episode, only the first reward that the agent receives at some specific state-action pair will be counted in the average. It is different from Every Visit MC, which counts in the average all rewards at all instances of a state-action pair in the episode.\n",
    "\n",
    "### MC Convergence\n",
    "MC methods leverage the Law of Large Numbers, which states that as the number of samples (episodes) increases, the sample averages converge to the true expected values. This convergence property ensures that, with enough exploration and sufficient episodes, MC estimates become increasingly accurate, leading to better approximations of value functions and policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e96aa3",
   "metadata": {},
   "source": [
    "# Temporal Difference Methods (TD)\n",
    "\n",
    "### Assumptions/Knowns\n",
    "- TD methods, like MC methods, don't need to have knowledge of the environment.\n",
    "### Objective\n",
    "- A table of action values. The objective is to continually improve these action value approximations until making moves greedily according to the values is the most optimal strategy.\n",
    "### Process\n",
    "- repeat for each episode:\n",
    "  - choose an action using epsilon greedy.\n",
    "  - Use the discounted action value at the next state (one-step TD) to update the value at the state-action pair.\n",
    "    - The update is (Q = Q + $\\alpha$ * TD error) where (TD error = TD target - Q) and TD target is (R + $\\gamma$ * Q')\n",
    "\n",
    "\n",
    "## Epsilon Greedy Action Selection\n",
    "TD methods often use epsilon-greedy action selection as an exploration strategy to balance the trade-off between exploration and exploitation in reinforcement learning. This exploration strategy is designed to help the agent learn and discover optimal or near-optimal policies while also ensuring that the agent continues to exploit its current knowledge of the environment to maximize cumulative rewards. Here's why TD methods commonly use epsilon-greedy action selection:\n",
    "\n",
    "- Exploitation: The \"greedy\" part of epsilon-greedy action selection ensures that the agent predominantly selects the action that is estimated to be the best (highest value) based on its current value estimates. This allows the agent to exploit its current knowledge and take actions that have been previously identified as promising.\n",
    "- Exploration: The \"epsilon\" part of epsilon-greedy action selection introduces a small probability (epsilon) that the agent will select a random action regardless of its current value estimates. This exploration component is crucial for the agent to explore new states and actions, which is essential for discovering better policies, avoiding getting stuck in suboptimal solutions, and handling uncertain or changing environments.\n",
    "- Convergence: Epsilon-greedy action selection can help the agent ensure convergence, especially when combined with certain TD algorithms like Q-learning. It encourages the agent to explore all possible actions in the long run, ensuring that the agent's value estimates converge to more accurate representations of the true values of the environment.\n",
    "- Balance: By tuning the value of epsilon, one can control the balance between exploration and exploitation. Higher epsilon values lead to more exploration, which can be useful at the early stages of learning when the agent's knowledge about the environment is limited. As the agent learns and refines its value estimates, epsilon can be gradually reduced, emphasizing exploitation of the most promising actions.\n",
    "- Simplicity: Epsilon-greedy action selection is a simple and intuitive exploration strategy. It doesn't require complex calculations or domain-specific knowledge, making it easy to implement and understand.\n",
    "- Robustness: Epsilon-greedy action selection is robust and generally effective across a wide range of reinforcement learning problems. While it might not be the most sophisticated exploration strategy, it often performs well in practice, especially when combined with TD learning methods.\n",
    "\n",
    "## SARSA\n",
    "SARSA is a reinforcement learning algorithm that is used for estimating the optimal action-value function (also known as the Q-function) in a Markov Decision Process (MDP). The name \"SARSA\" stands for the key components of the algorithm: State, Action, Reward, State', and Action'.\n",
    "\n",
    "SARSA belongs to the family of Temporal Difference (TD) learning algorithms and is closely related to Q-learning, another well-known algorithm for solving RL problems. The key difference between SARSA and Q-learning lies in their approach to learning the Q-function and the way they handle exploration.\n",
    "\n",
    "Here's a brief overview of the SARSA algorithm:\n",
    "- Initialization: Initialize the Q-function arbitrarily, often with small random values, for all state-action pairs in the MDP.\n",
    "- Select an Action: Given the current state (S), use an exploration strategy (often epsilon-greedy) to select an action (A) to take in the current state.\n",
    "- Take Action and Observe Reward and Next State: Execute the selected action, transition to a new state (S'), and observe the immediate reward (R) obtained from the environment.\n",
    "- Select Next Action: Use the same exploration strategy to select the next action (A') to take in the new state (S').\n",
    "- Update Q-function: Use the observed values (S, A, R, S', A') to update the Q-value for the current state-action pair. The update rule for SARSA is based on the following equation:\n",
    "  - Q(S, A) = Q(S, A) + α * [R + γ * Q(S', A') - Q(S, A)]\n",
    "  - Q(S, A): The Q-value for the current state-action pair.\n",
    "  - α (alpha): The learning rate, controlling the step size of the update.\n",
    "  - R: The observed immediate reward.\n",
    "  - γ (gamma): The discount factor, balancing immediate rewards with future rewards.\n",
    "  - Q(S', A'): The Q-value of the next state-action pair.\n",
    "- Repeat: Continue the process, selecting actions, observing rewards and transitions, and updating Q-values, until the agent reaches a terminal state or for a fixed number of time steps.\n",
    "\n",
    "The key idea in SARSA is that the Q-values are updated based on the action taken in the next state (A') as per the exploration policy. This means that SARSA is an on-policy method, as the policy used for learning (exploration) is the same as the policy being improved (exploitation).\n",
    "\n",
    "SARSA is particularly useful in scenarios where the agent's policy needs to be refined while it explores the environment, and it is well-suited for learning policies that balance exploration and exploitation.\n",
    "\n",
    "## Q Learning (SARSAMAX)\n",
    "Q-learning is a popular model-free reinforcement learning algorithm used for solving Markov Decision Processes (MDPs) by estimating the optimal action-value function (also known as the Q-function). Q-learning is a key algorithm in reinforcement learning because it enables an agent to learn an optimal policy for sequential decision-making tasks in an environment without requiring a model of the environment's transition probabilities.\n",
    "\n",
    "The primary goal of Q-learning is to find the optimal policy that maximizes the expected cumulative reward over time. The Q-function is a mapping from state-action pairs to the expected cumulative rewards the agent can achieve by starting in a particular state, taking a specific action, and then following an optimal policy thereafter.\n",
    "\n",
    "Q-learning is nearly exactly the same except for the update step. instead of getting an action to take in the next state using the policy, it uses the max action value in the next state. This is where the name SARSAMAX comes from.\n",
    "\n",
    "- Initialization: Initialize the Q-function arbitrarily for all state-action pairs in the MDP.\n",
    "- Select an Action: Given the current state (S), use an exploration strategy (often epsilon-greedy) to select an action (A) to take in the current state.\n",
    "- Take Action and Observe Reward and Next State: Execute the selected action, transition to a new state (S'), and observe the immediate reward (R) obtained from the environment.\n",
    "- Update Q-function: Use the observed values (S, A, R, S') to update the Q-value for the current state-action pair. The Q-value update rule for Q-learning is based on the following equation:\n",
    "  - Q(S, A) = Q(S, A) + α * [R + γ * max(Q(S', a)) - Q(S, A)]\n",
    "  - Q(S, A): The Q-value for the current state-action pair.\n",
    "  - α (alpha): The learning rate, controlling the step size of the update.\n",
    "  - R: The observed immediate reward.\n",
    "  - γ (gamma): The discount factor, balancing immediate rewards with future rewards.\n",
    "  - max(Q(S', a)): The maximum Q-value over all possible actions (a) in the next state (S').\n",
    "- Repeat: Continue the process, selecting actions, observing rewards and transitions, and updating Q-values, until the agent reaches a terminal state or for a fixed number of time steps.\n",
    "\n",
    "The key feature of Q-learning is its off-policy nature. It learns the optimal policy while following a separate policy for exploration, which can be different from the policy being improved. This allows the algorithm to learn an optimal policy without getting trapped in suboptimal solutions due to exploration.\n",
    "\n",
    "Q-learning is widely used in various applications, such as game playing, robotic control, and optimization problems, where the agent interacts with an environment to learn an optimal strategy over time.\n",
    "\n",
    "## Expected SARSA\n",
    "Expected SARSA is a variant of the SARSA (State-Action-Reward-State-Action) algorithm in reinforcement learning. Similar to SARSA, Expected SARSA is used to estimate the optimal action-value function (Q-function) in a Markov Decision Process (MDP) and to learn an optimal policy for sequential decision-making tasks in an environment.\n",
    "\n",
    "The main difference between Expected SARSA and the standard SARSA algorithm lies in the way they update the Q-values, particularly in the action selection step for the next state. In Expected SARSA, the update accounts for the expected value of the Q-function for the next state, taking into consideration the probabilities of taking each action under the current policy.\n",
    "\n",
    "- Initialization: Initialize the Q-function arbitrarily for all state-action pairs in the MDP.\n",
    "- Select an Action: Given the current state (S), use the current policy (which can be an exploration strategy like epsilon-greedy) to select an action (A) to take in the current state.\n",
    "- Take Action and Observe Reward and Next State: Execute the selected action, transition to a new state (S'), and observe the immediate reward (R) obtained from the environment.\n",
    "- Select Next Action Probabilities: Use the current policy to compute the probabilities of selecting each action (including the greedy action) in the next state (S').\n",
    "- Update Q-function: Use the observed values (S, A, R, S', probabilities of next actions) to update the Q-value for the current state-action pair. The Q-value update rule for Expected SARSA is based on the following equation:\n",
    "  - Q(S, A) = Q(S, A) + α * [R + γ * Σ(p(a|S') * Q(S', a)) - Q(S, A)]\n",
    "  - Q(S, A): The Q-value for the current state-action pair.\n",
    "  - α (alpha): The learning rate, controlling the step size of the update.\n",
    "  - R: The observed immediate reward.\n",
    "  - γ (gamma): The discount factor, balancing immediate rewards with future rewards.\n",
    "  - p(a|S'): The probability of selecting each action (a) in the next state (S') under the current policy.\n",
    "- Repeat: Continue the process, selecting actions, observing rewards and transitions, and updating Q-values, until the agent reaches a terminal state or for a fixed number of time steps.\n",
    "\n",
    "Expected SARSA is particularly useful in scenarios where a more stable update is desired compared to the often noisy updates of standard SARSA. The expected value of the Q-function in the update step provides a smoothed estimate of the future Q-value, resulting in more reliable learning, especially in environments with stochastic transitions or noisy rewards.\n",
    "\n",
    "## Convergence\n",
    "Temporal Difference (TD) methods converge due to their iterative nature and their ability to gradually refine value estimates based on observed experiences in the environment. The convergence of TD methods is a fundamental property that ensures that, over time, the value estimates approach the true values under certain conditions. Several factors contribute to the convergence of TD methods:\n",
    "\n",
    "Bellman Optimality Equation: TD methods are based on the Bellman optimality equation (or Bellman equation) in reinforcement learning. This equation expresses the optimal value of a state (or state-action pair) in terms of the immediate reward, the value of the next state (or next state-action pair), and the discount factor. By repeatedly applying this equation, TD methods iteratively update value estimates, gradually bringing them closer to the true values.\n",
    "\n",
    "Update Rules: TD methods use update rules that blend the current estimate of the value with a new estimate based on observed experiences. These update rules have a smoothing effect, preventing the value estimates from fluctuating wildly and helping them converge over time.\n",
    "\n",
    "Exploration and Sampling: TD methods rely on exploration to gather samples from the environment. As the agent explores different states and actions, it collects more information about the environment's dynamics and the rewards associated with different actions. This sampling process, combined with the update rules, helps the agent refine its value estimates and learn the optimal policy.\n",
    "\n",
    "Policy Improvement: In policy evaluation and control tasks, TD methods often work in conjunction with policy improvement mechanisms. These mechanisms, such as the greedy policy improvement step in Q-learning, help the agent select better actions over time based on the updated value estimates. This combination of policy improvement and value estimation contributes to the convergence of TD methods toward an optimal policy.\n",
    "\n",
    "It's important to note that the convergence of TD methods is influenced by various factors, such as the choice of the learning rate, the exploration strategy, the characteristics of the environment (e.g., whether it's episodic or continuing), and the specific TD algorithm being used (e.g., Q-learning, SARSA). In practice, the convergence of TD methods is not always guaranteed, especially in complex environments or when the learning rate is set improperly. However, when designed and tuned appropriately, TD methods can converge to near-optimal solutions in a wide range of reinforcement learning problems.\n",
    "\n",
    "- SARSA, Q-Learning (SARSA-MAX), Expected SARSA\n",
    "- blackjack (first 2), cliffwalk (all 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c10634",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MC Blackjack\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Blackjack-v1')\n",
    "\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "win = 0\n",
    "loss = 0\n",
    "draw = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(100000):\n",
    "    # reset the environment and see the initial state\n",
    "    state = env.reset()[0]\n",
    "    # print(\"The initial observation is {}\".format(obs))\n",
    "\n",
    "    while True:\n",
    "        # Sample a random action from the entire action space\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Take the action and get the new observation space\n",
    "        state, reward, terminated, _, _ = env.step(action)\n",
    "        # print(\"Take action {} new observation is {} reward {} terminated {} truncated {} info {}\".format(random_action, obs, reward, terminated, truncated, info))\n",
    "\n",
    "        if terminated:\n",
    "            if reward == 1:\n",
    "                win += 1\n",
    "            elif reward == 0:\n",
    "                draw += 1\n",
    "            else:\n",
    "                loss += 1\n",
    "            total += 1\n",
    "            break\n",
    "\n",
    "win_rate = win / total\n",
    "draw_rate = draw / total\n",
    "loss_rate = loss / total\n",
    "print(\"random move\")\n",
    "print(f\"{win_rate=},{draw_rate=},{loss_rate=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31deba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy):\n",
    "    win = 0\n",
    "    loss = 0\n",
    "    draw = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(100000):\n",
    "        # reset the environment and see the initial state\n",
    "        state = env.reset()[0]\n",
    "        # print(\"The initial observation is {}\".format(obs))\n",
    "\n",
    "        while True:\n",
    "            # Sample a random action from the entire action space\n",
    "            action = policy(state)\n",
    "\n",
    "            # Take the action and get the new observation space\n",
    "            state, reward, terminated, _, _ = env.step(action)\n",
    "            # print(\"Take action {} new observation is {} reward {} terminated {} truncated {} info {}\".format(random_action, obs, reward, terminated, truncated, info))\n",
    "\n",
    "            if terminated:\n",
    "                if reward == 1:\n",
    "                    win += 1\n",
    "                elif reward == 0:\n",
    "                    draw += 1\n",
    "                else:\n",
    "                    loss += 1\n",
    "                total += 1\n",
    "                break\n",
    "\n",
    "    win_rate = win / total\n",
    "    draw_rate = draw / total\n",
    "    loss_rate = loss / total\n",
    "    print(f\"{win_rate=},{draw_rate=},{loss_rate=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_blackjack_policy(policy, useable_ace):\n",
    "    print(\"useable ace\" if useable_ace else \"no useable ace\")\n",
    "    for hand in range(21,10,-1):\n",
    "        for dealer in range(1,11):\n",
    "            print(policy.get((hand, dealer, useable_ace), ' '), end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002063a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo ES\n",
    "\n",
    "policy = {}\n",
    "Q = {}\n",
    "returns = {}\n",
    "\n",
    "for i in range(500000):\n",
    "    state = env.reset()[0]\n",
    "    action = random.choice([0,1])\n",
    "    \n",
    "    episode = [state,action]\n",
    "    ret = 0\n",
    "    while True:\n",
    "        # Take the action and get the new observation\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode.append(state)\n",
    "        \n",
    "        if terminated:\n",
    "            ret = reward\n",
    "            break\n",
    "        \n",
    "        action = policy.get(state, 0 if state[0] >= 17 else 1)\n",
    "        episode.append(action)\n",
    "    \n",
    "    seen_pairs = set()\n",
    "    for p in range(len(episode) // 2):\n",
    "        s,a = episode[2*p], episode[2*p+1]\n",
    "        if (s,a) in seen_pairs: # continue makes this code first-visit MCES\n",
    "            continue\n",
    "        seen_pairs.add((s,a))\n",
    "        \n",
    "        G = ret\n",
    "        rets = returns.get((s,a),[])\n",
    "        rets.append(G)\n",
    "        returns[(s,a)] = rets\n",
    "        \n",
    "        Q[(s,a)] = sum(returns[(s,a)])/len(returns[(s,a)])\n",
    "    \n",
    "    for p in range(len(episode) // 2):\n",
    "        s = episode[2*p]\n",
    "        \n",
    "        action_values = []\n",
    "        for a in [0,1]:\n",
    "            action_values.append(Q.get((s,a),0))\n",
    "        \n",
    "        policy[s] = [0,1].index(np.argmax(action_values))\n",
    "\n",
    "MCES_policy = policy\n",
    "MCES_Q = Q\n",
    "print_blackjack_policy(policy,True)\n",
    "print_blackjack_policy(policy,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state,Q,epsilon,action_space):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return random.choice(action_space)\n",
    "    else:\n",
    "        q_values = []\n",
    "        for action in action_space:\n",
    "            q_values.append(Q[state,action])\n",
    "        return action_space[np.argmax(q_values)]\n",
    "\n",
    "def get_greedy_policy(Q,action_space):\n",
    "    policy = {}\n",
    "    for hand in range(32):\n",
    "        for dealer in range(11):\n",
    "            for useable_ace in range(2):\n",
    "                state = (hand, dealer, useable_ace)\n",
    "                q_values = []\n",
    "                for action in action_space:\n",
    "                    q_values.append(Q[state, action])\n",
    "                policy[state] = np.argmax(q_values)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3109094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA Blackjack\n",
    "\n",
    "action_space = [0,1]\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "gamma = 0.1\n",
    "Q = {((h,d,ua),a): 0 for h in range(32) for d in range(11) for ua in range(2) for a in range(2)}\n",
    "\n",
    "for i in range(500_000):\n",
    "    S = env.reset()[0]\n",
    "    A = epsilon_greedy_policy(S,Q,epsilon,action_space)\n",
    "    \n",
    "    while True:\n",
    "        # Take the action and get the new observation\n",
    "        S2, R, terminated, truncated, info = env.step(A)\n",
    "        A2 = epsilon_greedy_policy(S2,Q,epsilon,action_space)\n",
    "        \n",
    "        Q[S,A] = Q[S,A] + alpha * (R + gamma * Q[S2,A2] - Q[S,A])\n",
    "        \n",
    "        S = S2\n",
    "        A = A2\n",
    "        \n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "SARSA_policy = get_greedy_policy(Q,action_space)\n",
    "SARSA_Q = Q\n",
    "\n",
    "print_blackjack_policy(SARSA_policy,True)\n",
    "print_blackjack_policy(SARSA_policy,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52caaf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning Blackjack\n",
    "\n",
    "action_space = [0,1]\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "gamma = 0.1\n",
    "# !!!!! GAMMA is small because episodes are short. Shouldn't value the likely bust that is coming in 2 moves.\n",
    "Q = {((h,d,ua),a): 0 for h in range(32) for d in range(11) for ua in range(2) for a in range(2)}\n",
    "\n",
    "for i in range(500_000):\n",
    "    S = env.reset()[0]\n",
    "    \n",
    "    while True:\n",
    "        # get action from epsilon greedy\n",
    "        A = epsilon_greedy_policy(S,Q,epsilon,action_space)\n",
    "        \n",
    "        # Take the action and get the new observation\n",
    "        S2, R, terminated, truncated, info = env.step(A)\n",
    "        \n",
    "        q_values = []\n",
    "        for action in action_space:\n",
    "            q_values.append(Q.get((S2,action),0))\n",
    "        max_Q = np.max(q_values)\n",
    "\n",
    "        current_Q = Q.get((S,A),0)\n",
    "        Q[(S,A)] = (1 - alpha) * current_Q + alpha * (R + gamma * max_Q)\n",
    "        \n",
    "        S = S2\n",
    "        \n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "QLearning_policy = get_greedy_policy(Q,action_space)\n",
    "QLearning_Q = Q\n",
    "print_blackjack_policy(QLearning_policy,True)\n",
    "print_blackjack_policy(QLearning_policy,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5dd3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hit below 17, stick otherwise\")\n",
    "test_policy(lambda s: 0 if s[0] >= 17 else 1)\n",
    "print(\"MC Exploring Starts Policy\")\n",
    "test_policy(MCES_policy.get)\n",
    "print(\"SARSA Policy\")\n",
    "test_policy(SARSA_policy.get)\n",
    "print(\"Q Learning Policy\")\n",
    "test_policy(QLearning_policy.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa1f1b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cliffwalk\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CliffWalking-v0')\n",
    "\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22535039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state,Q,epsilon,action_space):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return random.choice(action_space)\n",
    "    else:\n",
    "        q_values = []\n",
    "        for action in action_space:\n",
    "            q_values.append(Q[state,action])\n",
    "        return action_space[np.argmax(q_values)]\n",
    "\n",
    "def get_greedy_policy(Q,action_space):\n",
    "    policy = {}\n",
    "    for state in range(48):\n",
    "        q_values = []\n",
    "        for action in action_space:\n",
    "            q_values.append(Q[state, action])\n",
    "        policy[state] = action_space[np.argmax(q_values)]\n",
    "\n",
    "    return policy\n",
    "\n",
    "def print_cliffwalk_policy(policy):\n",
    "    for y in range(4):\n",
    "        for x in range(12):\n",
    "            print(policy[12*y + x],end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34233523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA cliffwalk\n",
    "\n",
    "action_space = [0,1,2,3]\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "Q = {(s,a):0 for s in range(48) for a in range(4)}\n",
    "\n",
    "for i in range(50000):\n",
    "    S = env.reset()[0]\n",
    "    A = epsilon_greedy_policy(S,Q,epsilon,action_space)\n",
    "    \n",
    "    while True:\n",
    "        # Take the action and get the new observation\n",
    "        S2, R, terminated, truncated, info = env.step(A)\n",
    "        A2 = epsilon_greedy_policy(S2,Q,epsilon,action_space)\n",
    "        \n",
    "        Q[S,A] = Q[S,A] + alpha * (R + gamma * Q[S2,A2] - Q[S,A])\n",
    "        \n",
    "        S = S2\n",
    "        A = A2\n",
    "        \n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "SARSA_policy = get_greedy_policy(Q,action_space)\n",
    "SARSA_Q = Q\n",
    "print_cliffwalk_policy(SARSA_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4719a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning (SARSA MAX) cliffwalk\n",
    "\n",
    "action_space = [0,1,2,3]\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "Q = {(s,a):0 for s in range(48) for a in range(4)}\n",
    "\n",
    "for i in range(50000):\n",
    "    S = env.reset()[0]\n",
    "    \n",
    "    while True:\n",
    "        # get action from epsilon greedy\n",
    "        A = epsilon_greedy_policy(S,Q,epsilon,action_space)\n",
    "        # Take the action and get the new observation\n",
    "        S2, R, terminated, truncated, info = env.step(A)\n",
    "        \n",
    "        q_values = []\n",
    "        for action in action_space:\n",
    "            q_values.append(Q.get((S2,action),0))\n",
    "        max_Q = np.max(q_values)\n",
    "        current_Q = Q.get((S,A),0)\n",
    "        Q[S,A] = (1 - alpha) * current_Q + alpha * (R + gamma * max_Q)\n",
    "        \n",
    "        S = S2\n",
    "        \n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "QLearning_policy = get_greedy_policy(Q,action_space)\n",
    "QLearning_Q = Q\n",
    "print_cliffwalk_policy(QLearning_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected SARSA cliffwalk\n",
    "\n",
    "def epsilon_greedy_policy(state,Q,epsilon,action_space):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return random.choice(action_space)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def get_greedy_policy(Q,action_space):\n",
    "    policy = {}\n",
    "    for state in range(48):\n",
    "        policy[state] = np.argmax(Q[state])\n",
    "    return policy\n",
    "\n",
    "action_space = [0,1,2,3]\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "Q = {s:[0 for a in range(4)] for s in range(48)}\n",
    "\n",
    "for i in range(50000):\n",
    "    S = env.reset()[0]\n",
    "    \n",
    "    while True:\n",
    "        # get action from epsilon greedy\n",
    "        A = epsilon_greedy_policy(S,Q,epsilon,action_space)\n",
    "        # Take the action and get the new observation\n",
    "        S2, R, terminated, truncated, info = env.step(A)\n",
    "        \n",
    "        Q_max = np.max(Q[S2])\n",
    "        expected_Q = 0\n",
    "        \n",
    "        # count number of greedy actions possible if there are equally greedy actions\n",
    "        greedy_actions = 0\n",
    "        for a in action_space:\n",
    "            if Q[S2][a] == Q_max:\n",
    "                greedy_actions += 1\n",
    "        non_greedy_action_probability = epsilon / len(action_space)\n",
    "        greedy_action_probability = ((1 - epsilon) / greedy_actions) + non_greedy_action_probability\n",
    "        \n",
    "        for A2 in action_space:\n",
    "            expected_Q += Q[S2][A2] * (greedy_action_probability if Q[S2][A2] == Q_max else non_greedy_action_probability)\n",
    "        \n",
    "        td_target = R + gamma * expected_Q\n",
    "        td_error = td_target - Q[S][A]\n",
    "        Q[S][A] += alpha * (td_error)\n",
    "        \n",
    "        S = S2\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "E_SARSA_policy = get_greedy_policy(Q,action_space)\n",
    "E_SARSA_Q = Q\n",
    "print_cliffwalk_policy(E_SARSA_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659f7d9",
   "metadata": {},
   "source": [
    "# MC Tree Search\n",
    "(states increase a lot)\n",
    "\n",
    "### Assumptions/Knowns\n",
    "- Like the MC methods above, knowledge of the environment is not necessary.\n",
    "### Objective\n",
    "- A tree of action values. The objective is to continually improve these action value approximations until making moves greedily according to the action values is the most optimal strategy.\n",
    "### Process\n",
    "- Select a node, Expand a node, Simulate/Rollout starting from the node, Backup/Backpropagate the reward to all nodes traversed during selection.\n",
    "\n",
    "## Upper Confidence Bound applied to Trees (UCT) Action Selection\n",
    "Upper Confidence Bound (UCB) is a principle used in decision-making under uncertainty, and it's often applied in the context of tree-based algorithms, including Monte Carlo Tree Search (MCTS). UCB helps balance the exploration of unexplored options (nodes) with the exploitation of known good options, allowing the algorithm to effectively search through the state-action space while converging to a good solution.\n",
    "\n",
    "In the context of MCTS, the specific variant that utilizes UCB is often referred to as the \"Upper Confidence Bound for Trees\" or simply \"UCT.\" UCT is a key component of MCTS that guides the selection of child nodes during the simulation phase. The UCT formula combines two terms for each child node:\n",
    "\n",
    "- Exploration Term: This term encourages the algorithm to explore nodes that have been explored less frequently. It prevents the algorithm from getting stuck in local optima by giving priority to nodes that haven't been visited extensively.\n",
    "- Exploitation Term: This term accounts for the average value (estimated reward) of the child node. It encourages selecting actions that have shown promise in previous rollouts.\n",
    "\n",
    "The UCT formula can be expressed as follows:\n",
    "\n",
    "$$UCT(s) = \\frac{Q}{n_i} + c \\sqrt{\\frac{\\ln(N_i)}{n_i}} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $s$ is the child node being considered.\n",
    "- $Q$ is the current value estimate for s.\n",
    "- $n_i$ is the number of visits to the state s.\n",
    "- $c$ is the exploration weight, a hyperparameter that controls the balance between exploration and exploitation. A higher value encourages more exploration.\n",
    "- $N_i$ is the number of visits to the parent state of s.\n",
    "\n",
    "The UCT formula takes into account the trade-off between exploring less-visited nodes (controlled by the square root term) and exploiting nodes with higher average rewards (the first term). The logarithmic term ensures that less-visited nodes are not completely ignored, and the exploration weight allows you to adjust the balance based on the specific problem.\n",
    "\n",
    "By using UCT in the selection process during the expansion phase of MCTS, the algorithm tends to focus its exploration on promising but not fully explored regions of the state-action space, leading to efficient and effective decision-making.\n",
    "\n",
    "## MCTS\n",
    "Monte Carlo Tree Search (MCTS) is a popular algorithm used in reinforcement learning and game playing to efficiently explore and exploit the state-action space of a problem, particularly in scenarios where the complete information about the environment or game is not available upfront. MCTS is commonly applied to games like chess, Go, and various other board games, as well as to more general decision-making problems.\n",
    "\n",
    "MCTS works by building a search tree that represents the possible states and actions of the environment or game. The primary idea behind MCTS is to simulate multiple trajectories (or rollouts) from the current state to estimate the value of each action and then use this information to guide the decision-making process.\n",
    "\n",
    "Here's how the basic process of MCTS works:\n",
    "\n",
    "1. Selection: Starting from the root node (current state), the algorithm traverses down the tree by selecting child nodes based on certain criteria. The criteria typically balance between exploration and exploitation. A common approach is the Upper Confidence Bound for Trees (UCT) formula, which takes into account the exploration factor and the value of the node based on past experience. This process continues until a leaf node is reached.\n",
    "2. Expansion: Once a leaf node is reached, the algorithm determines if the node can be expanded (i.e., if it represents a state that hasn't been fully explored yet). If so, one or more child nodes are added to the tree, representing the possible actions that can be taken from that state.\n",
    "3. Simulation (Rollout): From the newly added child node, the algorithm performs a simulated trajectory (rollout) by randomly selecting actions until a terminal state is reached. The outcome of this simulation is typically a reward value.\n",
    "4. Backup/Backpropagation: The reward obtained from the simulation is backpropagated up the tree to update the value estimates of the nodes along the path from the root to the newly expanded node. This step aims to improve the value estimates of different actions in different states.\n",
    "- Repeat: Steps 1 to 4 are repeated for a predefined number of iterations or until a time limit is reached.\n",
    "\n",
    "Over multiple iterations, MCTS gradually refines its estimates of the value of each action in each state. The algorithm dynamically focuses its exploration on areas of the state-action space that seem promising based on the accumulated rewards and visitation counts. This allows MCTS to guide decision-making by choosing actions that are likely to lead to favorable outcomes, while also allocating some effort to explore potentially unexplored actions to avoid getting stuck in suboptimal solutions.\n",
    "\n",
    "MCTS works well in scenarios where the state space is large and not easily enumerable. It leverages the principles of exploration and exploitation to gradually converge towards better action selections. However, it's worth noting that MCTS is not limited to reinforcement learning for games; it can also be adapted for other decision-making problems, including those with partial observability and stochasticity.\n",
    "\n",
    "## No Convergence Guarantees\n",
    "Monte Carlo Tree Search (MCTS) is a powerful algorithm for decision-making in certain domains, particularly in games and similar scenarios with large state-action spaces. However, it's important to note that MCTS is not guaranteed to find an optimal solution or even converge to a good solution in all cases. There are several reasons why MCTS may not always work or why its performance might be limited:\n",
    "\n",
    "1. **Exploration Challenges:** While MCTS balances exploration and exploitation to gradually refine its value estimates, it's possible for the algorithm to get stuck in suboptimal branches of the search tree due to insufficient exploration, especially if the exploration weight (used in UCT) is not set properly.\n",
    "\n",
    "2. **Limited Search Depth:** The effectiveness of MCTS can be influenced by the search depth (number of iterations or playouts). If the search depth is too limited, the algorithm might not have enough time to explore and converge to optimal or near-optimal solutions, especially in complex problems.\n",
    "\n",
    "3. **High Dimensionality:** In high-dimensional state-action spaces, MCTS may struggle to efficiently explore all relevant areas, leading to incomplete or biased value estimates.\n",
    "\n",
    "4. **Sample Variability:** MCTS relies on sampling and rollouts to estimate the value of actions. In some cases, the inherent randomness in these samples can lead to inaccurate or noisy value estimates, affecting the quality of the decision-making.\n",
    "\n",
    "5. **Noisy Rewards:** In situations where the rewards are noisy or have a high variance, MCTS may struggle to differentiate between good and bad actions due to the uncertainty in reward estimation.\n",
    "\n",
    "6. **Intractable Models:** If the underlying model of the environment or game is intractable or highly complex, MCTS might have difficulty accurately representing the state transitions and rewards, leading to suboptimal performance.\n",
    "\n",
    "7. **Complex Dynamics:** If the problem involves non-stationary or dynamically changing environments, MCTS may not adapt quickly enough to capture these changes, leading to suboptimal decision-making.\n",
    "\n",
    "Despite these limitations, MCTS is a valuable and widely used algorithm, and it has achieved impressive results in various domains, particularly in game playing. Its strength lies in its ability to handle large state spaces and its potential for finding good solutions in complex decision-making problems. However, its performance depends on the specific characteristics of the problem, the quality of the underlying model, the search depth, and other factors. It's important to carefully tune MCTS parameters, monitor its performance, and consider its limitations when applying it to real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a88ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TicTacToe MCTS\n",
    "import copy\n",
    "\n",
    "class TicTacToe:\n",
    "    height,width = 3,3\n",
    "    def __init__(self, board=None, player='X'):\n",
    "        self.board = [[' ' for _ in range(TicTacToe.width)] for _ in range(TicTacToe.height)] if board == None else copy.deepcopy(board)\n",
    "        self.player = player\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '\\n-----\\n'.join('|'.join(row) for row in self.board)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(''.join(c for r in self.board for c in r) + self.player)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if other == None:\n",
    "            return False\n",
    "        s = ''.join(c for r in self.board for c in r) + self.player\n",
    "        o = ''.join(c for r in other.board for c in r) + other.player\n",
    "        return s == o\n",
    "    \n",
    "    def can_move(self, row, col):\n",
    "        return self.board[row][col] == ' '\n",
    "    \n",
    "    def move(self, row, col):\n",
    "        assert self.can_move(row, col)\n",
    "        self.board[row][col] = self.player\n",
    "        next_state = TicTacToe(board=self.board, player=('O' if self.player == 'X' else 'X'))            \n",
    "        self.board[row][col] = ' '\n",
    "        return next_state\n",
    "    \n",
    "    def board_full(self):\n",
    "        return all(self.board[r][c] != ' ' for r in range(TicTacToe.height) for c in range(TicTacToe.width))\n",
    "\n",
    "    def available_moves(self):\n",
    "        return [(row,col) for row in range(TicTacToe.height) for col in range(TicTacToe.width) if self.can_move(row,col)]\n",
    "    \n",
    "    def next_states(self):\n",
    "        return set(self.move(r,c) for r,c in self.available_moves()) if not self.is_terminal() else set()\n",
    "    \n",
    "    def check_win(self):\n",
    "        lines = [\n",
    "            [(0,0),(0,1),(0,2)],\n",
    "            [(1,0),(1,1),(1,2)],\n",
    "            [(2,0),(2,1),(2,2)],\n",
    "            [(0,0),(1,0),(2,0)],\n",
    "            [(0,1),(1,1),(2,1)],\n",
    "            [(0,2),(1,2),(2,2)],\n",
    "            [(0,0),(1,1),(2,2)],\n",
    "            [(2,0),(1,1),(0,2)]\n",
    "        ]\n",
    "        \n",
    "        for line in lines:\n",
    "            all_same = True\n",
    "            cmp_r,cmp_c = line[0]\n",
    "            for r,c in line:\n",
    "                if self.board[r][c] != self.board[cmp_r][cmp_c]:\n",
    "                    all_same = False\n",
    "            if all_same and self.board[cmp_r][cmp_c] != ' ':\n",
    "                return True, self.board[cmp_r][cmp_c]\n",
    "\n",
    "        return False, (\"tie\" if self.board_full() else None)\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        return self.check_win()[0] or self.board_full()\n",
    "    \n",
    "    def reward(self):\n",
    "        if not self.is_terminal():\n",
    "            raise Exception(\"Cannot reward non-terminal state\")\n",
    "        is_win, winner = self.check_win()\n",
    "        if is_win:\n",
    "            return 1.0 if winner == ('X' if self.player == 'O' else 'O') else 0\n",
    "        else:\n",
    "            return 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, initial_state):\n",
    "        self.initial_state = initial_state\n",
    "        self.table = dict()\n",
    "        self.expanded = dict()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.table)\n",
    "\n",
    "    def UCT(self, state, c=1):\n",
    "        # state should always be in the table when calling UCT. Otherwise there's an error\n",
    "        assert all(n in self.expanded for n in self.expanded[state])\n",
    "        N, V = self.table[state]\n",
    "        def uct(s):\n",
    "            n,v = self.table.get(s,(0,0))\n",
    "            return v/n + c * np.sqrt(np.log(N)/n)\n",
    "        return max(self.expanded[state], key=uct)\n",
    "    \n",
    "    def select(self, state):\n",
    "        path = []\n",
    "        while True:\n",
    "            path.append(state)\n",
    "            if state not in self.expanded or not self.expanded[state]:\n",
    "                return path\n",
    "            \n",
    "            for child in self.expanded[state]:\n",
    "                if child not in self.expanded:\n",
    "                    path.append(child)\n",
    "                    return path\n",
    "            \n",
    "            state = self.UCT(state,c=1) # UCT\n",
    "            \n",
    "    def expand(self, state):\n",
    "        if state in self.expanded: return\n",
    "        self.expanded[state] = state.next_states()\n",
    "\n",
    "    def simulate(self, state):\n",
    "        while True:\n",
    "            if state.is_terminal():\n",
    "                return state.reward()\n",
    "            action = random.choice(state.available_moves())\n",
    "            state = state.move(*action)\n",
    "\n",
    "    def backup(self, path, reward):\n",
    "        while path != []:\n",
    "            state = path.pop()\n",
    "            N, V = self.table.get(state, (0,0))\n",
    "            self.table[state] = (N+1, V+reward)\n",
    "            reward = 1-reward\n",
    "\n",
    "    def SESB(self, num_rollout=1):\n",
    "        # select\n",
    "        path = self.select(self.initial_state)      \n",
    "        leaf = path[-1]\n",
    "        # expand\n",
    "        self.expand(leaf)\n",
    "        # simulate\n",
    "        reward = 0\n",
    "        for _ in range(num_rollout):\n",
    "            reward += self.simulate(leaf)\n",
    "        # backup/backpropagate\n",
    "        self.backup(path, reward)\n",
    "\n",
    "    def next(self, state):\n",
    "        if state.is_terminal():\n",
    "            raise Exception(\"Can't call next on terminal state\")\n",
    "        \n",
    "        if state not in self.expanded:\n",
    "            action = random.choice(state.available_moves())\n",
    "            return state.move(*action)\n",
    "        \n",
    "        def score(s):\n",
    "            n,v = self.table.get(s,(0,0))\n",
    "            return float(\"-inf\") if n == 0 else v/n\n",
    "    \n",
    "        return max(self.expanded[state], key=score)\n",
    "        \n",
    "    def runMCTS(self, num_iter=1000):\n",
    "        for _ in range(num_iter):\n",
    "            self.SESB()\n",
    "#         print(self.table)\n",
    "        print(len(self.expanded), \"visits\")\n",
    "\n",
    "ttt_mcts1k = MCTS(TicTacToe())\n",
    "ttt_mcts1k.runMCTS(1000)\n",
    "ttt_mcts2k = MCTS(TicTacToe())\n",
    "ttt_mcts2k.runMCTS(2000)\n",
    "ttt_mcts3k = MCTS(TicTacToe())\n",
    "ttt_mcts3k.runMCTS(3000)\n",
    "ttt_mcts4k = MCTS(TicTacToe())\n",
    "ttt_mcts4k.runMCTS(4000)\n",
    "ttt_mcts5k = MCTS(TicTacToe())\n",
    "ttt_mcts5k.runMCTS(5000)\n",
    "ttt_mcts10k = MCTS(TicTacToe())\n",
    "ttt_mcts10k.runMCTS(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_ttt(agent1,agent2=None):\n",
    "    ttt = TicTacToe()\n",
    "    print(ttt)\n",
    "    while not ttt.is_terminal():\n",
    "        if ttt.player == 'X':\n",
    "            print(\"mcts: X\")\n",
    "            ttt = agent1.next(ttt)\n",
    "            print(ttt)\n",
    "        elif agent2:\n",
    "            print(\"mcts: O\")\n",
    "            ttt = agent2.next(ttt)\n",
    "            print(ttt)\n",
    "        else:\n",
    "            print(\"you: O\")\n",
    "            r,c = int(input()), int(input())\n",
    "            ttt = ttt.move(r,c)\n",
    "            print(ttt)\n",
    "    win,winner=ttt.check_win()\n",
    "    print(winner + (\" wins\" if win else \"\"))\n",
    "\n",
    "play_ttt(ttt_mcts5k,ttt_mcts1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mcts(agent, iters=10000):\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    losses = 0\n",
    "    for _ in range(iters):\n",
    "        ttt = TicTacToe()\n",
    "        while not ttt.is_terminal():\n",
    "            if ttt.player == 'X':\n",
    "#                 print(\"mcts: X\")\n",
    "                ttt = agent.next(ttt)\n",
    "#                 print(ttt)\n",
    "            else:\n",
    "#                 print(\"random: O\")\n",
    "                ttt = random.choice(list(ttt.next_states()))\n",
    "#                 print(ttt)\n",
    "        win,winner=ttt.check_win()\n",
    "        if winner == 'X':\n",
    "            wins += 1\n",
    "        elif winner == 'O':\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "#         print(winner + (\" wins\" if win else \"\"))\n",
    "    \n",
    "    print(f\"wins: {wins}/{iters}\")\n",
    "    print(f\"draws: {draws}/{iters}\")\n",
    "    print(f\"losses: {losses}/{iters}\")\n",
    "\n",
    "# test_mcts(ttt_mcts1k)\n",
    "# test_mcts(ttt_mcts2k)\n",
    "# test_mcts(ttt_mcts3k)\n",
    "# test_mcts(ttt_mcts4k)\n",
    "# test_mcts(ttt_mcts5k)\n",
    "test_mcts(ttt_mcts10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61839837",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connect4 MCTS\n",
    "import copy\n",
    "\n",
    "class Connect4:\n",
    "    height,width = 6,7\n",
    "    def __init__(self, board=None, player='@', terminal=False, winner=None):\n",
    "        self.board = [[' ' for _ in range(Connect4.width)] for _ in range(Connect4.height)] if board == None else copy.deepcopy(board)\n",
    "        self.player = player\n",
    "        self.terminal = terminal\n",
    "        self.winner = winner\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"\\n{'-'*(2*Connect4.width-1)}\\n\".join('|'.join(row) for row in self.board)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(''.join(c for r in self.board for c in r) + self.player)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if other == None:\n",
    "            return False\n",
    "        s = ''.join(c for r in self.board for c in r) + self.player\n",
    "        o = ''.join(c for r in other.board for c in r) + other.player\n",
    "        return s == o\n",
    "    \n",
    "    def can_move(self, col):\n",
    "        return self.board[0][col] == ' '\n",
    "    \n",
    "    def move(self, col):\n",
    "        assert self.can_move(col)\n",
    "        row = -1\n",
    "        while (row+1) in range(Connect4.height) and self.board[row+1][col] == ' ':\n",
    "            row += 1\n",
    "        self.board[row][col] = self.player\n",
    "        win, winner = self.check_win(row, col)\n",
    "        terminal = win or self.board_full()\n",
    "        next_state = Connect4(board=self.board, player=('O' if self.player == '@' else '@'), terminal=terminal, winner=winner)            \n",
    "        self.board[row][col] = ' '\n",
    "        return next_state\n",
    "    \n",
    "    def board_full(self):\n",
    "        return all(self.board[r][c] != ' ' for r in range(Connect4.height) for c in range(Connect4.width))\n",
    "\n",
    "    def available_moves(self):\n",
    "        return [(col,) for col in range(Connect4.width) if self.can_move(col)]\n",
    "    \n",
    "    def next_states(self):\n",
    "        return set(self.move(*c) for c in self.available_moves()) if not self.is_terminal() else set()\n",
    "    \n",
    "    def check_win(self, row, col):\n",
    "        in_a_row = 4\n",
    "        player = self.board[row][col]\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = col + offset_column * i\n",
    "                if (r not in range(Connect4.height) or c not in range(Connect4.width) or self.board[r][c] != player):\n",
    "                    return i - 1\n",
    "            return in_a_row - 1\n",
    "\n",
    "        if (count(1, 0) >= in_a_row - 1\n",
    "            or (count(0, 1) + count(0, -1)) >= in_a_row - 1\n",
    "            or (count(1, 1) + count(-1, -1)) >= in_a_row - 1\n",
    "            or (count(1, -1) + count(-1, 1)) >= in_a_row - 1) :\n",
    "            return True, player\n",
    "        else:\n",
    "            return False, (\"tie\" if self.board_full() else None)\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        return self.terminal\n",
    "    \n",
    "    def reward(self):\n",
    "        if not self.is_terminal():\n",
    "            raise Exception(\"Cannot reward non-terminal state\")\n",
    "        if self.winner:\n",
    "            return 1.0 if self.winner == ('O' if self.player == '@' else '@') else 0\n",
    "        else:\n",
    "            return 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "022ac3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_c4(agent1,agent2=None):\n",
    "    c4 = Connect4()\n",
    "    print(c4)\n",
    "    while not c4.is_terminal():\n",
    "        if c4.player == '@':\n",
    "            print(\"mcts: @\")\n",
    "            c4 = agent1.next(c4)\n",
    "            print(c4)\n",
    "        elif agent2:\n",
    "            print(\"mcts: O\")\n",
    "            c4 = agent2.next(c4)\n",
    "            print(c4)\n",
    "        else:\n",
    "            print(\"you: O\")\n",
    "            while True:\n",
    "                try:\n",
    "                    c = int(input())\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            c4 = c4.move(c)\n",
    "            print(c4)\n",
    "    print(c4.winner + \" wins\" if c4.winner else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba48e0",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods\n",
    "\n",
    "Policy gradient algorithms are well-suited for solving a wide range of reinforcement learning problems, particularly those that involve continuous action spaces, stochastic policies, and scenarios where the agent needs to explore and adapt its behavior to maximize cumulative rewards. Here are some features and types of problems for which policy gradient algorithms are particularly effective:\n",
    "\n",
    "1. **Continuous Action Spaces**: Policy gradient methods naturally handle problems where the action space is continuous, such as controlling robotic systems, autonomous vehicles, or other systems with a large number of possible actions at each step.\n",
    "\n",
    "2. **Stochastic Policies**: Policy gradient algorithms can learn stochastic policies, which are policies that output probability distributions over actions rather than deterministic actions. This can be beneficial when there's uncertainty in the optimal action to take.\n",
    "\n",
    "3. **Exploration**: Policy gradient methods often incorporate exploration mechanisms to encourage the agent to explore new actions and states, which is crucial for learning in complex and uncertain environments. Stochastic policies and exploration techniques (e.g., adding noise to the policy) help the agent explore various actions to discover the best strategy.\n",
    "\n",
    "4. **High-Dimensional State Spaces**: Policy gradient methods can handle high-dimensional state spaces, which is important for tasks where the agent needs to process large amounts of sensory input, such as images or sensor readings.\n",
    "\n",
    "5. **Sample Efficiency**: While policy gradient methods can require a reasonable amount of data to learn good policies, they can be more sample-efficient than some other reinforcement learning approaches, such as Q-learning, in certain scenarios. Techniques like advantage estimation and importance sampling can further improve sample efficiency.\n",
    "\n",
    "6. **Non-Differentiable Reward Functions**: In some cases, the reward function might not be differentiable or easy to optimize with gradient-based methods. Policy gradient algorithms can often handle such cases by directly optimizing the policy in the space of actions.\n",
    "\n",
    "7. **Multi-Agent and Cooperative Settings**: Policy gradient methods can be extended to handle multi-agent and cooperative settings, where multiple agents interact in the same environment and may need to learn joint strategies.\n",
    "\n",
    "It's important to note that while policy gradient methods have these specific features, they are not limited to these scenarios. They can also be applied to discrete action spaces, deterministic policies, and other types of reinforcement learning problems. Additionally, the choice between policy gradient methods and other RL approaches depends on the specific characteristics of the problem and the trade-offs between exploration, sample efficiency, and convergence speed.\n",
    "\n",
    "### Assumptions/Knowns\n",
    "Many policy gradient methods rely on the assumption that the policy's parameterized function is differentiable with respect to its parameters. This differentiability is necessary for computing the gradient of the policy with respect to the parameters, which is crucial for policy updates.\n",
    "### Objective\n",
    "Policy Gradient methods also try to maximize the Expected Cumulative Reward (Expected Return)\n",
    "$$J(\\theta) = E[\\Sigma_{t=0}^{T-1} γ^t r_{t+1}]$$\n",
    "\n",
    "- J(θ) is the objective function to be maximized, which is a function of the policy's parameters θ.\n",
    "- t represents the time step within a trajectory, ranging from 0 to T-1.\n",
    "- γ (gamma) is the discount factor, which is a value between 0 and 1 that determines the importance of future rewards. It discounts the value of rewards that are further in the future.\n",
    "- $r_{t+1}$ is the reward received after time step t in the trajectory.\n",
    "\n",
    "To update the parameters $\\theta$, take a step in the direction of the gradient of $J(\\theta)$.\n",
    "\n",
    "### Process\n",
    "Initialize policy parameters, use the policy to run the agent, calculate the gradient, and use the gradient to update the parameters. \n",
    "\n",
    "\n",
    "## REINFORCE\n",
    "The REINFORCE algorithm is one of the fundamental policy gradient methods used in reinforcement learning. It is a simple and intuitive algorithm for optimizing the policy of an agent to maximize the expected cumulative reward. REINFORCE uses the Monte Carlo method to estimate the gradient of the expected cumulative reward with respect to the policy parameters and then updates the policy parameters to improve performance.\n",
    "\n",
    "Here's an overview of the REINFORCE algorithm:\n",
    "\n",
    "1. **Initialize Policy**: Initialize the policy, often parameterized by θ (the policy parameters).\n",
    "\n",
    "2. **Collect Trajectories**: Interact with the environment using the current policy to collect a batch of trajectories. A trajectory is a sequence of states, actions, and rewards that the agent experiences during one episode or a fixed number of time steps.\n",
    "\n",
    "3. **Compute Cumulative Returns**: For each trajectory, compute the cumulative return G(t) for each time step t in the trajectory. The cumulative return is the sum of rewards from time step t to the end of the trajectory, possibly discounted by a factor γ (the discount factor) to account for the importance of future rewards.\n",
    "\n",
    "4. **Compute Policy Gradient**: For each trajectory, compute the policy gradient for each time step t using the following formula:\n",
    "\n",
    "   ∇θ J(θ) = ∑[t=0 to T] ∇θ log π(a_t|s_t, θ) * G(t),\n",
    "\n",
    "   where:\n",
    "   - ∇θ J(θ) is the policy gradient with respect to the policy parameters θ.\n",
    "   - π(a_t|s_t, θ) is the probability of taking action a_t in state s_t under the current policy parameterized by θ.\n",
    "   - G(t) is the cumulative return for time step t.\n",
    "\n",
    "5. **Update Policy Parameters**: Perform a parameter update using the estimated policy gradient to improve the policy:\n",
    "\n",
    "   θ_new = θ_old + α * ∇θ J(θ),\n",
    "\n",
    "   where:\n",
    "   - θ_new is the updated policy parameter.\n",
    "   - θ_old is the current policy parameter.\n",
    "   - α is the learning rate, controlling the step size in the parameter update.\n",
    "\n",
    "6. **Repeat**: Iteratively collect trajectories, compute policy gradients, and update the policy parameters until the policy converges or a stopping criterion is met.\n",
    "\n",
    "REINFORCE is a powerful conceptually simple algorithm, but it has some limitations, such as high variance in the gradient estimates, which can lead to slow convergence, and the requirement for a large number of trajectories to obtain accurate gradient estimates. Variations and improvements, such as using baseline functions to reduce variance (e.g., subtracting a state-dependent baseline from the returns) and using more advanced optimization techniques, have been developed to address these issues.\n",
    "\n",
    "\n",
    "## Actor Critic\n",
    "The Actor-Critic algorithm is a popular reinforcement learning technique that combines elements of both value-based and policy-based approaches. It's an extension of the basic policy gradient methods like REINFORCE but includes an additional component called the \"critic,\" which estimates the value function. The actor-critic architecture allows for more stable and efficient policy optimization by leveraging both policy information (from the actor) and value information (from the critic). The update step is similar to that of TD methods. The advantage calculation is bootstrapping.\n",
    "\n",
    "Here's an overview of the Actor-Critic algorithm:\n",
    "\n",
    "1. **Actor**: The \"actor\" is the policy that the agent aims to optimize. It's often parameterized by θ (policy parameters). The actor is responsible for selecting actions based on the current state to maximize the expected cumulative reward.\n",
    "\n",
    "2. **Critic**: The \"critic\" is a value function estimator that evaluates the quality of the current policy. It estimates the expected cumulative reward that the agent can achieve starting from a given state and following the policy. The critic helps the actor by providing a value signal that indicates how good the current policy is.\n",
    "\n",
    "3. **Collect Trajectories**: Interact with the environment using the current policy (actor) to collect trajectories. A trajectory is a sequence of states, actions, and rewards experienced during one episode or a fixed number of time steps.\n",
    "\n",
    "4. **Compute Value Estimates**: For each state encountered in the collected trajectories, the critic estimates the value function, which represents the expected cumulative reward from that state following the current policy.\n",
    "\n",
    "5. **Compute Advantage**: The advantage function represents how much better (or worse) the cumulative reward is compared to the average reward, starting from the current state and taking the current policy into account. It's computed as the difference between the cumulative return (G) and the estimated value from the critic (V(s)).\n",
    "  - Advantage(s, a) = G - V(s), where s is the state, a is the action, G is the cumulative return, and V(s) is the critic's estimate of the value of the state s.\n",
    "  - Critic Update:\n",
    "    - Update the critic (value function) using a mean squared error loss between the estimated value and the cumulative return or discounted reward. The update is performed to minimize the discrepancy between the value estimates and the actual returns:\n",
    "    - L = (G - V(s))^2.\n",
    "    - The goal of this update is to bring the critic's value estimates closer to the actual rewards the agent received, which helps provide a more accurate value signal to the actor for policy updates.\n",
    "  - Actor Update:\n",
    "    - The policy (actor) is updated using the policy gradient computed with the advantage estimates:\n",
    "    - ∇θ J(θ) ≈ ∑[t=0 to T] ∇θ log π(a_t|s_t, θ) * Advantage(s_t, a_t).\n",
    "\n",
    "6. **Compute Policy Gradient**: The policy gradient is computed similarly to the REINFORCE algorithm, but it's now augmented with the advantage information to reduce the variance in gradient estimates:\n",
    "\n",
    "   ∇θ J(θ) ≈ ∑[t=0 to T] ∇θ log π(a_t|s_t, θ) * Advantage(s_t, a_t).\n",
    "\n",
    "7. **Update Policy and Critic**: Perform updates for both the policy (actor) and the value function (critic) using the policy gradient and the estimated advantage. The actor updates its parameters to improve the policy, and the critic updates its parameters to improve its value estimates.\n",
    "\n",
    "8. **Repeat**: Iteratively collect trajectories, compute advantage, compute policy gradient, and update both the actor and the critic until convergence or a stopping criterion is met.\n",
    "\n",
    "The actor-critic approach has several advantages over basic policy gradient methods:\n",
    "\n",
    "- It reduces variance in gradient estimates by using value estimates as a baseline.\n",
    "- It enables the agent to learn more efficiently, especially in high-dimensional state spaces.\n",
    "- It allows for continuous updates, providing a better balance between exploration and exploitation.\n",
    "- It can handle both discrete and continuous action spaces.\n",
    "\n",
    "There are various variations and improvements to the actor-critic algorithm, such as the Advantage Actor-Critic (A2C) and asynchronous versions like A3C (Asynchronous Advantage Actor-Critic). These variations help to address issues like stability, convergence, and sample efficiency.\n",
    "\n",
    "## Convergence\n",
    "REINFORCE and Actor-Critic methods converge under certain conditions due to the use of the policy gradient approach and the incorporation of value estimates, respectively. The convergence of these methods is a consequence of the underlying optimization principles and the fact that they aim to maximize the expected cumulative reward over time. However, it's important to note that convergence is not guaranteed in all cases, and the specific implementation and learning setup can affect the convergence properties.\n",
    "\n",
    "Here's a high-level explanation of why REINFORCE and Actor-Critic methods can converge:\n",
    "\n",
    "1. **REINFORCE**:\n",
    "   - REINFORCE uses the policy gradient approach to optimize the policy directly. It computes the gradient of the expected cumulative reward with respect to the policy parameters and performs gradient ascent on these parameters to improve the policy.\n",
    "   - Convergence of REINFORCE is guaranteed under the assumption of:\n",
    "     - A finite action space (or specific techniques to handle continuous action spaces).\n",
    "     - Continuous exploration (ensuring that all actions are explored with non-zero probability).\n",
    "     - The use of a suitable learning rate schedule.\n",
    "   - The convergence proof of REINFORCE is based on the stochastic approximation theory, which provides conditions under which the updates to the policy parameters lead to convergence to a locally optimal policy.\n",
    "\n",
    "2. **Actor-Critic**:\n",
    "   - Actor-Critic combines the policy gradient approach (actor) with value function estimation (critic). The critic provides value estimates that guide the actor's policy updates, reducing the variance in gradient estimates.\n",
    "   - The use of value function estimates helps stabilize the learning process and allows the actor to focus on actions that are expected to lead to higher cumulative rewards, thus aiding in the convergence of the policy.\n",
    "   - Convergence of Actor-Critic depends on similar assumptions as REINFORCE, such as the use of a suitable learning rate, exploration, and proper handling of continuous action spaces (if applicable).\n",
    "   - The combination of the actor (policy) and the critic (value function) contributes to more stable updates and can lead to faster convergence compared to some purely policy-based methods.\n",
    "\n",
    "While REINFORCE and Actor-Critic methods have theoretical foundations that suggest convergence under certain conditions, in practice, the choice of hyperparameters, architecture design, exploration strategies, and specific implementation details can significantly impact the convergence behavior. In complex environments, achieving convergence may require careful tuning and experimentation. Researchers continue to work on developing more stable and efficient reinforcement learning algorithms to improve the convergence properties of these methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7dbcd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CartPole-v1 REINFORCE\n",
    "## Taken from PyTorch examples on github.\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "args = {\"gamma\": 0.99, \"seed\": 543, \"render\": True, \"log_interval\": 10}\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = deque()\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + args['gamma'] * R\n",
    "        returns.appendleft(R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    step_counts = []\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            if args['render']:\n",
    "                env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                step_counts.append(t)\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % args['log_interval'] == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "    plt.plot(step_counts)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f0ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CartPole-v1 Actor-Critic (A2C)\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(Actor, self).__init__()\n",
    "        self.actor1 = nn.Linear(env.observation_space.shape[0] , 128)\n",
    "        self.actor2 = nn.Linear(128, env.action_space.n)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.actor2(torch.relu(self.actor1(x))), dim=-1)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(Critic, self).__init__()\n",
    "        self.critic1 = nn.Linear(env.observation_space.shape[0] , 128)\n",
    "        self.critic2 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.critic2(torch.relu(self.critic1(x)))\n",
    "\n",
    "args = {\"gamma\": 0.99, \"seed\": 543, \"render\": True, \"log_interval\": 10, \"actor_lr\": 1e-3, \"critic_lr\": 1e-3}\n",
    "gamma = args['gamma']\n",
    "seed = args['seed']\n",
    "render = args['render']\n",
    "log_interval = args['log_interval']\n",
    "actor_lr = args['actor_lr']\n",
    "critic_lr = args['critic_lr']\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "actor = Actor(env)\n",
    "critic = Critic(env)\n",
    "actor_optim = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "critic_optim = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "\n",
    "def policy(state):\n",
    "    action_probs = actor(state)\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample()\n",
    "    return action, m.log_prob(action)\n",
    "\n",
    "def main():\n",
    "    step_counts = []\n",
    "    last_actor_losses = []\n",
    "    last_critic_losses = []\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        S, _ = env.reset()\n",
    "        S = torch.tensor(S, requires_grad=True).float().unsqueeze(0)\n",
    "        \n",
    "        ep_reward = 0\n",
    "        I = 1\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            A, log_prob = policy(S)\n",
    "            S2, R, done, _, _ = env.step(A.item())\n",
    "            S2 = torch.tensor(S2, requires_grad=True).float().unsqueeze(0)\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "            ep_reward += R\n",
    "            \n",
    "            error = R + (1.0 - done) * gamma * critic(S2) - critic(S)\n",
    "            error.requires_grad_()\n",
    "\n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss = - log_prob * error #* I\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            actor_optim.step()\n",
    "            \n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss = torch.square(error)\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            critic_optim.step()\n",
    "            \n",
    "            I = gamma * I\n",
    "            S = S2\n",
    "            \n",
    "            if done:\n",
    "                step_counts.append(t)\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "    plt.plot(step_counts)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
