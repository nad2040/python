{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751b114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b516aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Discrete(48)\n",
      "The action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CliffWalking-v0')\n",
    "\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0f0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state,Q,epsilon,action_space):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return random.choice(action_space)\n",
    "    else:\n",
    "        q_values = []\n",
    "        for action in action_space:\n",
    "            q_values.append(Q[state,action])\n",
    "        return action_space[np.argmax(q_values)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f918f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_policy(Q,action_space):\n",
    "    policy = {}\n",
    "    for state in range(48):\n",
    "        q_values = []\n",
    "        for action in action_space:\n",
    "            q_values.append(Q[state, action])\n",
    "        policy[state] = action_space[np.argmax(q_values)]\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db8e52dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cliffwalk_policy(policy):\n",
    "    for y in range(4):\n",
    "        for x in range(12):\n",
    "            print(policy[12*y + x],end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa232ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111111111122\n",
      "003300310112\n",
      "003000000302\n",
      "000000000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SARSA\n",
    "\n",
    "action_space = [0,1,2,3]\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "Q = {(s,a):0 for s in range(48) for a in range(4)}\n",
    "\n",
    "for i in range(50000):\n",
    "    S = env.reset()[0]\n",
    "    A = epsilon_greedy_policy(S,Q,epsilon,action_space)\n",
    "    \n",
    "    while True:\n",
    "        # Take the action and get the new observation\n",
    "        S2, R, terminated, truncated, info = env.step(A)\n",
    "        A2 = epsilon_greedy_policy(S2,Q,epsilon,action_space)\n",
    "        \n",
    "        Q[S,A] = Q[S,A] + alpha * (R + gamma * Q[S2,A2] - Q[S,A])\n",
    "        \n",
    "        S = S2\n",
    "        A = A2\n",
    "        \n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "SARSA_policy = get_greedy_policy(Q,action_space)\n",
    "SARSA_Q = Q\n",
    "print_cliffwalk_policy(SARSA_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11fe7093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111111111112\n",
      "111111111112\n",
      "111111111112\n",
      "000000000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q Learning (SARSA MAX)\n",
    "\n",
    "action_space = [0,1,2,3]\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "Q = {(s,a):0 for s in range(48) for a in range(4)}\n",
    "\n",
    "for i in range(50000):\n",
    "    S = env.reset()[0]\n",
    "    \n",
    "    while True:\n",
    "        # get action from epsilon greedy\n",
    "        A = epsilon_greedy_policy(S,Q,epsilon,action_space)\n",
    "        # Take the action and get the new observation\n",
    "        S2, R, terminated, truncated, info = env.step(A)\n",
    "        \n",
    "        q_values = []\n",
    "        for action in action_space:\n",
    "            q_values.append(Q.get((S2,action),0))\n",
    "        max_Q = np.max(q_values)\n",
    "        current_Q = Q.get((S,A),0)\n",
    "        Q[S,A] = (1 - alpha) * current_Q + alpha * (R + gamma * max_Q)\n",
    "        \n",
    "        S = S2\n",
    "        \n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "QLearning_policy = get_greedy_policy(Q,action_space)\n",
    "QLearning_Q = Q\n",
    "print_cliffwalk_policy(QLearning_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f91a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111111111112\n",
      "111111111112\n",
      "000000000012\n",
      "000000000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Expected SARSA\n",
    "\n",
    "def epsilon_greedy_policy(state,Q,epsilon,action_space):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return random.choice(action_space)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def get_greedy_policy(Q,action_space):\n",
    "    policy = {}\n",
    "    for state in range(48):\n",
    "        policy[state] = np.argmax(Q[state])\n",
    "    return policy\n",
    "\n",
    "action_space = [0,1,2,3]\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "Q = {s:[0 for a in range(4)] for s in range(48)}\n",
    "\n",
    "for i in range(50000):\n",
    "    S = env.reset()[0]\n",
    "    \n",
    "    while True:\n",
    "        # get action from epsilon greedy\n",
    "        A = epsilon_greedy_policy(S,Q,epsilon,action_space)\n",
    "        # Take the action and get the new observation\n",
    "        S2, R, terminated, truncated, info = env.step(A)\n",
    "        \n",
    "        Q_max = np.max(Q[S2])\n",
    "        expected_Q = 0\n",
    "        \n",
    "        # count number of greedy actions possible if there are equally greedy actions\n",
    "        greedy_actions = 0\n",
    "        for a in action_space:\n",
    "            if Q[S2][a] == Q_max:\n",
    "                greedy_actions += 1\n",
    "        non_greedy_action_probability = epsilon / len(action_space)\n",
    "        greedy_action_probability = ((1 - epsilon) / greedy_actions) + non_greedy_action_probability\n",
    "        \n",
    "        for A2 in action_space:\n",
    "            expected_Q += Q[S2][A2] * (greedy_action_probability if Q[S2][A2] == Q_max else non_greedy_action_probability)\n",
    "        \n",
    "        td_target = R + gamma * expected_Q\n",
    "        td_error = td_target - Q[S][A]\n",
    "        Q[S][A] += alpha * (td_error)\n",
    "        \n",
    "        S = S2\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "E_SARSA_policy = get_greedy_policy(Q,action_space)\n",
    "E_SARSA_Q = Q\n",
    "print_cliffwalk_policy(E_SARSA_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db2f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
