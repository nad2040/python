{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b212512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "The action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Blackjack-v1')\n",
    "\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a529d1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win_rate=0.28207,draw_rate=0.04042,loss_rate=0.67751\n"
     ]
    }
   ],
   "source": [
    "win = 0\n",
    "loss = 0\n",
    "draw = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(100000):\n",
    "    # reset the environment and see the initial state\n",
    "    state = env.reset()[0]\n",
    "    # print(\"The initial observation is {}\".format(obs))\n",
    "\n",
    "    while True:\n",
    "        # Sample a random action from the entire action space\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Take the action and get the new observation space\n",
    "        state, reward, terminated, _, _ = env.step(action)\n",
    "        # print(\"Take action {} new observation is {} reward {} terminated {} truncated {} info {}\".format(random_action, obs, reward, terminated, truncated, info))\n",
    "\n",
    "        if terminated:\n",
    "            if reward == 1:\n",
    "                win += 1\n",
    "            elif reward == 0:\n",
    "                draw += 1\n",
    "            else:\n",
    "                loss += 1\n",
    "            total += 1\n",
    "            break\n",
    "\n",
    "win_rate = win / total\n",
    "draw_rate = draw / total\n",
    "loss_rate = loss / total\n",
    "print(\"random move\")\n",
    "print(f\"{win_rate=},{draw_rate=},{loss_rate=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc50d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy):\n",
    "    win = 0\n",
    "    loss = 0\n",
    "    draw = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(100000):\n",
    "        # reset the environment and see the initial state\n",
    "        state = env.reset()[0]\n",
    "        # print(\"The initial observation is {}\".format(obs))\n",
    "\n",
    "        while True:\n",
    "            # Sample a random action from the entire action space\n",
    "            action = policy(state)\n",
    "\n",
    "            # Take the action and get the new observation space\n",
    "            state, reward, terminated, _, _ = env.step(action)\n",
    "            # print(\"Take action {} new observation is {} reward {} terminated {} truncated {} info {}\".format(random_action, obs, reward, terminated, truncated, info))\n",
    "\n",
    "            if terminated:\n",
    "                if reward == 1:\n",
    "                    win += 1\n",
    "                elif reward == 0:\n",
    "                    draw += 1\n",
    "                else:\n",
    "                    loss += 1\n",
    "                total += 1\n",
    "                break\n",
    "\n",
    "    win_rate = win / total\n",
    "    draw_rate = draw / total\n",
    "    loss_rate = loss / total\n",
    "    print(f\"{win_rate=},{draw_rate=},{loss_rate=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c414b044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win_rate=0.41197,draw_rate=0.10077,loss_rate=0.48726\n"
     ]
    }
   ],
   "source": [
    "test_policy(lambda s: 0 if s[0] >= 17 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1daaaa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_blackjack_policy(policy, useable_ace):\n",
    "    print(\"useable ace\" if useable_ace else \"no useable ace\")\n",
    "    for hand in range(21,10,-1):\n",
    "        for dealer in range(1,11):\n",
    "            print(policy.get((hand, dealer, useable_ace), ' '), end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8e6e723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "useable ace\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0001100011\n",
      "1111111111\n",
      "1111111111\n",
      "1111111111\n",
      "1111111111\n",
      "1111111111\n",
      "1111111111\n",
      "          \n",
      "\n",
      "no useable ace\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "1000000000\n",
      "1000001111\n",
      "1000001111\n",
      "1000001111\n",
      "1000001111\n",
      "1110101111\n",
      "1111111111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo ES\n",
    "\n",
    "policy = {}\n",
    "Q = {}\n",
    "returns = {}\n",
    "\n",
    "for i in range(500000):\n",
    "    state = env.reset()[0]\n",
    "    action = random.choice([0,1])\n",
    "    \n",
    "    episode = [state,action]\n",
    "    ret = 0\n",
    "    while True:\n",
    "        # Take the action and get the new observation\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode.append(state)\n",
    "        \n",
    "        if terminated:\n",
    "            ret = reward\n",
    "            break\n",
    "        \n",
    "        action = policy.get(state, 0 if state[0] >= 17 else 1)\n",
    "        episode.append(action)\n",
    "    \n",
    "    seen_pairs = set()\n",
    "    for p in range(len(episode) // 2):\n",
    "        s,a = episode[2*p], episode[2*p+1]\n",
    "        if (s,a) in seen_pairs: # continue makes this code first-visit MCES\n",
    "            continue\n",
    "        seen_pairs.add((s,a))\n",
    "        \n",
    "        G = ret\n",
    "        rets = returns.get((s,a),[])\n",
    "        rets.append(G)\n",
    "        returns[(s,a)] = rets\n",
    "        \n",
    "        Q[(s,a)] = sum(returns[(s,a)])/len(returns[(s,a)])\n",
    "    \n",
    "    for p in range(len(episode) // 2):\n",
    "        s = episode[2*p]\n",
    "        \n",
    "        action_values = []\n",
    "        for a in [0,1]:\n",
    "            action_values.append(Q.get((s,a),0))\n",
    "        \n",
    "        policy[s] = [0,1].index(np.argmax(action_values))\n",
    "\n",
    "MCES_policy = policy\n",
    "MCES_Q = Q\n",
    "print_blackjack_policy(policy,True)\n",
    "print_blackjack_policy(policy,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "012af95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state,Q,epsilon,action_space):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return random.choice(action_space)\n",
    "    else:\n",
    "        q_values = []\n",
    "        for action in action_space:\n",
    "            q_values.append(Q[state,action])\n",
    "        return action_space[np.argmax(q_values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "699aa1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_policy(Q,action_space):\n",
    "    policy = {}\n",
    "    for hand in range(32):\n",
    "        for dealer in range(11):\n",
    "            for useable_ace in range(2):\n",
    "                state = (hand, dealer, useable_ace)\n",
    "                q_values = []\n",
    "                for action in action_space:\n",
    "                    q_values.append(Q[state, action])\n",
    "                policy[state] = np.argmax(q_values)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "737ba4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "useable ace\n",
      "0000000000\n",
      "1000000000\n",
      "1110100011\n",
      "1101111111\n",
      "1111111111\n",
      "1111111111\n",
      "1111111111\n",
      "1111111111\n",
      "1111111111\n",
      "1111111111\n",
      "0000000000\n",
      "\n",
      "no useable ace\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "1000000000\n",
      "1100001111\n",
      "1001001111\n",
      "1101101111\n",
      "1110101011\n",
      "1111011111\n",
      "1111111111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SARSA\n",
    "\n",
    "action_space = [0,1]\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "gamma = 0.1\n",
    "Q = {((h,d,ua),a): 0 for h in range(32) for d in range(11) for ua in range(2) for a in range(2)}\n",
    "\n",
    "for i in range(500_000):\n",
    "    S = env.reset()[0]\n",
    "    A = epsilon_greedy_policy(S,Q,epsilon,action_space)\n",
    "    \n",
    "    while True:\n",
    "        # Take the action and get the new observation\n",
    "        S2, R, terminated, truncated, info = env.step(A)\n",
    "        A2 = epsilon_greedy_policy(S2,Q,epsilon,action_space)\n",
    "        \n",
    "        Q[S,A] = Q[S,A] + alpha * (R + gamma * Q[S2,A2] - Q[S,A])\n",
    "        \n",
    "        S = S2\n",
    "        A = A2\n",
    "        \n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "SARSA_policy = get_greedy_policy(Q,action_space)\n",
    "SARSA_Q = Q\n",
    "\n",
    "print_blackjack_policy(SARSA_policy,True)\n",
    "print_blackjack_policy(SARSA_policy,False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a943afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "useable ace\n",
      "0000000000\n",
      "1000000000\n",
      "1100110011\n",
      "1111100111\n",
      "1111110111\n",
      "1111111111\n",
      "1111111111\n",
      "1101111111\n",
      "1111111111\n",
      "1111111111\n",
      "0000000000\n",
      "\n",
      "no useable ace\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "0000000000\n",
      "1000000010\n",
      "1000000111\n",
      "1000001111\n",
      "1101111111\n",
      "1111101111\n",
      "1011111111\n",
      "1111111111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q Learning\n",
    "\n",
    "action_space = [0,1]\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "gamma = 0.1\n",
    "# !!!!! GAMMA is small because episodes are short. Shouldn't value the likely bust that is coming in 2 moves.\n",
    "Q = {((h,d,ua),a): 0 for h in range(32) for d in range(11) for ua in range(2) for a in range(2)}\n",
    "\n",
    "for i in range(500_000):\n",
    "    S = env.reset()[0]\n",
    "    \n",
    "    while True:\n",
    "        # get action from epsilon greedy\n",
    "        A = epsilon_greedy_policy(S,Q,epsilon,action_space)\n",
    "        \n",
    "        # Take the action and get the new observation\n",
    "        S2, R, terminated, truncated, info = env.step(A)\n",
    "        \n",
    "        q_values = []\n",
    "        for action in action_space:\n",
    "            q_values.append(Q.get((S2,action),0))\n",
    "        max_Q = np.max(q_values)\n",
    "\n",
    "        current_Q = Q.get((S,A),0)\n",
    "        Q[(S,A)] = (1 - alpha) * current_Q + alpha * (R + gamma * max_Q)\n",
    "        \n",
    "        S = S2\n",
    "        \n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "QLearning_policy = get_greedy_policy(Q,action_space)\n",
    "QLearning_Q = Q\n",
    "print_blackjack_policy(QLearning_policy,True)\n",
    "print_blackjack_policy(QLearning_policy,False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca0b1aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win_rate=0.43242,draw_rate=0.0897,loss_rate=0.47788\n",
      "win_rate=0.42572,draw_rate=0.09437,loss_rate=0.47991\n",
      "win_rate=0.42629,draw_rate=0.09154,loss_rate=0.48217\n"
     ]
    }
   ],
   "source": [
    "test_policy(MCES_policy.get)\n",
    "test_policy(SARSA_policy.get)\n",
    "test_policy(QLearning_policy.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0f01e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
